\chapter{Turing Machines}
\begin{quotation}\textit{
One of the biggest difference between a calculator and a computer is that
the computer has a serious amount of memory. When one talks about using a
computer to solve some problem -- say sorting -- implicitly that should
mean that you consider not just sorting a set of 10 values but that
you look at processing as many as anybody could ever provide. This
sets an agenda that the key thing that a model for computation must
provide is memory. Turing machines do this by having as their most
important feature a ``tape'' where data lives on it as a sequence of
symbols. The tape is assumed to be long enough for whatever task is to
be performed. This is one of the most important basic abtractions
of what computers are and hence a model in which it is possible to
explore what they can and can not do and how much time or space
they may need to do it.
}\end{quotation}
There have been two or three big reasons for looking for minimal
schemes that are able -- in some sense -- to compute.
\begin{enumerate}
\item If you want to build a computer it is very reasonable to try to
make things easy for yourself by designing the simplest possible device
that you can. Even though that might make life impressively harder for
those who want to write programs for it. There are modest applications
of this idea that have led to very successful commercial designs the
successors of which are in widespread use today -- but the extremist
version of it may provide a great basis for a fun practical construction
project. The assertion ``I have made by own computer from scratch'' is
obviously a good one to be able to make;
\item If you are serious about wanting to develop a robust theory that
{\em really} lets you understand what computers can do and what they can
not it is rational to start with something simple. What you will be
trying to do will be difficuly enough without needing to worry whether
all the special capabilities built into modern computers and programming
languages make big differences;
\item Those who are concerned with how long it should take to solve some
problem will find there are huge and shifting complications if they
consider hours. minutes and seconds on a range of desktop and laptop
machines as well as mobile phones and embedded controllers. By looking
at timings on truly reduced hardware their results will be less of immediate
relevance but can be ones that will remain valid as this year's
computers are replaced by next years ones that are posisbly from a quite
different manufacturer.
\end{enumerate}

The best known minimalist sort of computer is the Turing Machine. This
emphasises the fact that a computer of any sort will have memory and if
viewed from a distance all that it does can be seen as taking steps
that each inspect and change a single item within that memory. In focussing
on how data is stored it does not take any steps to make it particularly
easy to coax it into solving the problem that interests you.

The storage provided by a Turing Machine is a tape which is marked out in
cells. Each cell can hold one of a modest number of symbols. The number
of symbolc allowed in one of the parameters that describe exactly what
sort of Turing machine is being considered. The tape is made long enough
for whatever task you are happening to try to process. Some people would
characterise that as a tape that is infinite in length, or would use
the word ``unbounded'', but in any particular computation that the Turing
Machine takes only a finite segment of it will be used. So for practical
experimentation it can be acceptable to provide a limited length ``tape''
and view an attempt to go beyond its end as merely reaching a limitation of
the physical approximation to the abstract machine.

The Turing machine starts with whatever input data it needs ready on the
tape. It then works step by step: it has a read/write head positioned
over some call of the tape and a cycle it takes will inspect the
symbol there and based on an internal state (from a limited number) it
will write back a possbly changed symbol and move the tape left or
right. It also transfers into some internal state. One state will be
special in that entering it causes the maching to halt. At that stage it
is expected that it will be written its result onto the tape. The number of
distinct states that the machine can be in is the second parameter alongside
the size of the alphabet of symbols on the tape that characterise it.

Programming a Turing machine has to involve setting up a table that
is indexed by which symbol has just been read and which state the machine
is in. When that information is used to inspect a row in the table
you can read off the symbol to be written, the tape movement to apply and
the identify of the next state that the machine should be in.  A reasonable
expectation is that designing tables like that to perform even modestly
elaborate computations will be a bit painful.

One compelling reason that theoreticians like Turing Machines is that
any such machines has its entire behaviour defined by this table, and it
is easy to write out such tables on paper, set them up in computer
programs for simulation and to set them out as sequences of symbols
written on a tape.

One way to prove that it is really worthwhile setting up this fairly
clumsy looking model of computation is a proof by construction that it
makes it feasible to build a mechanism that follows its behaviour
patter. In LEGO!

\begin{verbatim}
https://beta.ideas.lego.com/product-ideas/
   10a3239f-4562-4d23-ba8e-f4fc94eef5c7
\end{verbatim}
is a concrete realisation of this using under 3000 LEGO parts. While that is
quite a lot, it can be put in perspective by comparing with the official
LEGO kit to make a model of the Star Wars Death Star, which comes with
9023 pieces -- but rather fewer gearwheels. And an astounding thing is that
the only limitation at all on what thie Lego Turiing machine could do is
set by the length of its ``tape'' and at least in imagination it would be
easy to make that really long.

When considering building a real working Turing Machine it is reasonable to
ask just how much mechanism it needs to have before it can actually
perform any useful calculations. The LEGO version has 8 states and
handles an alphabet of 4 symbols on its tape and that feels as if it might be
quite limiting, however the astounding thing is that if you had a long
enough tape (and seeing how to make the tape a {\em bit} longer is surely not
a terribly tricky technical challenge even if giving it a capacity of
thousands or millions of cells starts to seem absurd) this is enough to
allow the machine to perform {\bf any} computation that any other computer
can. To be more specific it will be possible to set up initial contents on the
tape where the first part amounts to ``program'' that documents what is
to be done and the rest is the ``input data''. A very special case of this
is that the program part can explain how to behave as if the rest of the
tape is being used by a Turing machine with a larger number of states
and a bigger alphabet. For instance if one wanted to have 8-bit
characters on the tape the machine that would be emulated would
systematically use the contents of four consecutive cells on the physical
tape to represent a single byte on the bigger system. The details of
making one Turing Machine emulate another even if the latter is
large really amount to nothing more than messy programming. If necessary
multiple cells on the real tape will be grouped together so they can be
treated as if they were single cells using a larger range of symbols. Some
part of the tape will be used to record the current state of the emulated
machine, and a significant part of the fixed data on the tape will be
dedicated to a table recording how transitions happen. To orchestrate
all this the head of the baseline Turing Machine will be very busy
zooming backwards and forwards between the location of the active point on
the simulated tape, the record of the state of the simulated machine and the
transition table. It will be possible for it to find each of those by
marking their locations with special symbols that are not used elsewhere.
Details are not given here because setting it up it is closer to boring
details programming work than to anything especially exciting.

However designing, understanding and using Turing machines that are
really close to being mimimal can be huge fun. The sort of fun that
there so often can be in seeking the most compact way to achieve a
task! But really small universal Turing Machines can sometimes demand
ugly ways of encoding the data on their tape or suffer from needing
unreasonably large numbers of steps to reach results. Subject to that
issue it is amazing how small a machine can be while still being capable
of emulating larger machines and hence solving any problems that those
larger ones could.

A machine using just 2 states and 3 symbols exists and is characterised
as ``almost weakly universal''. It will not even halt when it has
completed its work and it also needs its tape initialised to
a particular but non-repeating  pattern in all regions beyond the input data,
while standard machines will not read from the tape beyond the region that is
explicitly data - it also tends to take a great many steps to get
anywhere, while modestly larger machines can actually be quite efficient.
As benchmarks for what is possible it is known that with just 2 states but
an alphabet of 18 symbols universality can be attained, and similarly
fpor just 2 symbols and 19 states. Between those 5 symbolc and 5 states
is also sufficient -- much work has gone into charting the boundary and
discussing exactly what rules should apply.



Once one has demonstrated that a basic and fairly small Turing Machine
can be used to emulate a machine with more states or a larger alphabet
it can be reasonable for subsequent work to feel free to relax those
constraints to make machine design simpler. and in particular at least
from a theoretical perspective it is acceptable to imagine that the
``state'' part of the machine can do anything that an ``ordinary
computer'' with no unbounded memory can. The tape is then just
needed to provide the unbounded storage that it is good to have
when analysing algorithms. To slightly simplify the proper and general result,
if the ``ordinary computer'' completes a run within $N$ steps it can
not possibly have touched more than $N$ distict memory locations. If
its memory is modelled by data on the tape then the most remote
bit of data ever accessed can be no more than about $N$ cells away.
Well we may aggregate raw tape cells so that some symbols are stored
spanning across say $K$ of them, but then the furthest accessed data
is only $K N$ away, and the Turing machine should snly take time
proportional to that to access it. So we find that each of the $N$ steps
of the ordinary machine gets emulated in time bounded by something of
the form $K N^2$. This quadratic overhead would of couse be calamitous in
practise, but is modest enough for a great many theoretical studies.


Rather simple extensions and generalisations to Turing Machines allow
for yet better efficiency for many problems. One can consider a
device wither with more than one tape or with just one tape but more than
one read/write head.  As a concrete example of how this makes things easier,
Merge Sort was a solid solution to sorting vast amounts of data
when computer memory was small and the data has to live on magnetic tapes.
One version of it worked by starting with the unsorted data split
between two tapes and at each step it compared the two items next visible
on those tapes and transferred the larger to an output tape. By the end of
the pass the data originally on the two tapes had been merged. Well a proper
version would handle not just a pair of items at once but as much as
it could reasonably fit into its memory. With that sort of scheme if
the total amount of data was about $N$ times larger than the computer's
memory capacity the sorting could be completes after only $K \log N$ passes
for some fairly small value of the constant $K$. 
The treatment of those tapes and those in a multi-tape Turing machine
are closely analagous to one another, so a multi-tape TM provides a
really solid model for analysing that particular algorithm while avoiding
the need to worry about detailed characteristics of any physical
computer. Similar reasoning applies to enough other tasks that Turing
machines pretty much set the gold standard for properly pedantic
discussion of assymptotic growth rates in cost.

The overall message here is that if you really want to make things
hard for yourself try to design the most compact Turing Machine
that can do the calculation you are interested in, seeing how you
can make trade-offs between the number of states, the size of the
alphabet you allow on the tape, the ugliness of how data has to be
coded for use and just how many compoutational steps will be taken
to obtain your solution. That is a wild and confusing space to
search within. A variation on this is just to look for Turing machines
that run for a seriously long time but then stop when they are started
on an empty tape. For really small machines bounds are known. For instance
if the tape can only hold one of two symbols in each call and the
machine has 2, 3, 4 or 5 states the number of steps it might take
before termination can be 6, 21, 107 and 47176870. With more states
the number of steps becomes infeasible to write using commonly-used
notations! These results show rather clearly that very small systems
can have extraordinarily complex behaviour and is systems as small
as these can behave in such extraordinary ways the detailed understanding of
larger and less artifial ones will be difficult in the extreme.

Perhaps the most important resully regarding Turing Machines is that if you
are given one there is no systematic procedure you could ever have that
would guarantee to tell you if it was going to terminate. This result
emerges from the techniques that allow one machine to emulate another
in that thet task starts by representaing the second machine in the
form of data that cen be processed by the first. It imagines there was
a ``systematic procedure'' that could take such a description and judge
whether it represented a machine that woudl terminate or not. It is
pretty obvious that it could just emulate the machine and if that led
to a termination state it has an answer. The problem will be that it has
no bound on how long it might need to run the simulation for and as a
result it is really hard for it to be certain that the emulated machine
is never going to terminate rather than it just having a very long run-time.

One big trick is used! Given the imagined ``will this machine terminate''
machine (it is perhaps useful to think of it as a program and the
patterns on the tape that describe it as its source code) one
makes a trick version. This (if it worked) would decide if its input
was a terminating program and in that case it would go into a boring
infinite loop. If on the other hand it determined that its input was a
machine that was going to run for ever it reports that fact by halting.
You give a description of this new machine to inself as its input data.
Now because of the trickery upi have something that will terminate
only if it does not, and vice versa. Ooops that is impossible! And the
only assumption that had been made was that there existed a Turing Machine
that could determine if another one was going to halt. The inescapable
conclusion is that such a machine can not exist. Phrased in the way
this is normally spoken on ``The Halting Problem is Undecidable''. Given
this result it is possible to deduce that many other questions about
programs can not always be answered. That does not mean that in particular
instances it may not be possible to resolve them -- just that no
effective procedure that canm cope with every case can exist. Well if
(in general) one can not tell if a Turing Machine (or program) is ever going
to terminate it is going to be impossible to tell if two programs
always compute the same output (in particular one might sometimes halt
when the other does not). In turn that means that guaranteeing to find
that smallest program that achieves just what a reference one would can
not be done automatically. These results pretty well leave resolving those
sorts of questions in particular cases as begin ``difficult''.

To finish this chapter it is proper to mention another variation on
these machines that has enough witty consequences that it will form the
basis of a whole separate chapter. Ordinary Turing machines are very
much mechanical and deterministic devices that always behave in unambiguous
and predictable ways. An entertaining variation will be the class
of machines where the state transition is not quite deterministic. In
some cases the machine will be able to choose for itself from two
different next states, with no pre-programmed or external guidance.
If the choice was made by notionally tossing a coin in each such
case you would have a randomised machine, and exploring the capabilities
there would be entertaining. But the most important case here is where
the decision between the alternate paths is made as if some good fairy
waves a magic wand and the computation proceeds such that if your
calculation could at all possibly succeed it now will. This is obviously
a delightful fantasy, and the resulting model of computation is
referred to as a ``non-deterministic Turing Machine''. It seems obvious
that machines like this could never exist in reality -- but in fact
if you abandon all concern for timings they could be emulated, and
to date nobody has been able to prove that there is no way of building
a rather efficient simulation of one.


