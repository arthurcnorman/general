\chapter{Turing Machines}
There have been two or three big reasons for looking for minimal
schemes that are able -- in some sense -- to compute.
\begin{enumerate}
\item If you want to build a computer it is very reasonable to try to
make things easy for yourself by designing the simplest possible device
that you can. Even though that might make life impressively harder for
those who want to write programs for it. There are modest applications
of this idea that have led to very successful commercial designs the
successors of which are in widespread use today -- but the extremist
version of it may provide a great basis for a fun practical construction
project. The assertion ``I have made by own computer from scratch'' is
obviously a good one to be able to make;
\item If you are serious about wanting to develop a robust theory that
{\em really} lets you understand what computers can do and what they can
not it is rational to start with something simple. What you will be
trying to do will be difficuly enough without needing to worry whether
all the special capabilities built into modern computers and programming
languages make big differences;
\item Those who are concerned with how long it should take to solve some
problem will find there are huge and shifting complications if they
consider hours. minutes and seconds on a range of desktop and laptop
machines as well as mobile phones and embedded controllers. By looking
at timings on truly reduced hardware their results will be less of immediate
relevance but can be ones that will remain valid as this year's
computers are replaced by next years ones that are posisbly from a quite
different manufacturer.
\end{enumerate}

The best known minimalist sort of computer is the Turing Machine. This
emphasises the fact that a computer of any sort will have memory and if
viewed from a distance all that it does can be seen as taking steps
that each inspect and change a single item within that memory. In focussing
on how data is stored it does not take any steps to make it particularly
easy to coax it into solving the problem that interests you.

The storage provided by a Turing Machine is a tape which is marked out in
cells. Each cell can hold one of a modest number of symbols. The number
of symbolc allowed in one of the parameters that describe exactly what
sort of Turing machine is being considered. The tape is made long enough
for whatever task you are happening to try to process. Some people would
characterise that as a tape that is infinite in length, or would use
the word ``unbounded'', but in any particular computation that the Turing
Machine takes only a finite segment of it will be used. So for practical
experimentation it can be acceptable to provide a limited length ``tape''
and view an attempt to go beyond its end as merely reaching a limitation of
the physical approximation to the abstract machine.

The Turing machine starts with whatever input data it needs ready on the
tape. It then works step by step: it has a read/write head positioned
over some call of the tape and a cycle it takes will inspect the
symbol there and based on an internal state (from a limited number) it
will write back a possbly changed symbol and move the tape left or
right. It also transfers into some internal state. One state will be
special in that entering it causes the maching to halt. At that stage it
is expected that it will be written its result onto the tape. The number of
distinct states that the machine can be in is the second parameter alongside
the size of the alphabet of symbols on the tape that characterise it.

Programming a Turing machine has to involve setting up a table that
is indexed by which symbol has just been read and which state the machine
is in. When that information is used to inspect a row in the table
you can read off the symbol to be written, the tape movement to apply and
the identify of the next state that the machine should be in.  A reasonable
expectation is that designing tables like that to perform even modestly
elaborate computations will be a bit painful.

One way to prove that it is really worthwhile setting up this fairly
clumsy looking model of computation is a proof by construction that it
makes it feasible to build a mechanism that follows its behaviour
patter. In LEGO!

\begin{verbatim}
https://beta.ideas.lego.com/product-ideas/
   10a3239f-4562-4d23-ba8e-f4fc94eef5c7
\end{verbatim}
is a concrete realisation of this using under 3000 LEGO parts. While that is
quite a lot, it can be put in perspective by comparing with the official
LEGO kit to make a model of the Star Wars Death Star, which comes with
9023 pieces -- but rather fewer gearwheels. And an astounding thing is that
the only limitation at all on what thie Lego Turiing machine could do is
set by the length of its ``tape'' and at least in imagination it would be
easy to make that really long.

When considering building a real working Turing Machine it is reasonable to
ask just how much mechanism it needs to have before it can actually
perform any useful calculations. The LEGO version has 8 states and
handles an alphabet of 4 symbols on its tape and that feels as if it might be
quite limiting, however the astounding thing is that if you had a long
enough tape (and seeing how to make the tape a bit longer is surely not
a terribly tricky technical challenge) this is enough to allow the
machine to perform {\bf any} computation that any other computer can.
To be more specific it will be possible to set up initial contents on the
tape where the first part amounts to ``program'' that documents what is
to be done and the rest is the ``input data''. A very special case of this
is that the program part can explain how to behave as if the rest of the
tape is being used by a Turing machine with a larger number of states
and a bigger alphabet. For instance if one wanted to have 8-bit
characters on the tape the machine that would be emulated would
systematically use the contents of four consecutive cells on the physical
tape to represent a single byte on the bigger system. Designing,
understanding and using Turing machines that are really close to being
mimimal can be huge fun however they can sometimes demand really ugly
ways of encoding the data on their tape or suffer from needing
unreasonably large numbers of steps to reach results.

A variant using just 2 states and 3 symbols exists and is characterised
as ``weakly universal''. It will not even halt when it has
completed its work and it also needs its tape initialised to
a particular pattern in all regions beyond the input data, while standard
machines will not read from the tape beyong the region that is
explicitly data - it also tends to take a great many steps to get
anywhere, while modestly larger machines are actually quite efficient.

Once one has demonstrated that a basic and fairly small Turing Machine
can be use dto emulate a machine with more states or a larger alphabet
it can be reasonable for subsequent work to feel free to relax those
constraints to make machine design simpler. and in particular at least
from a theoretical perspective it is acceptable to imagine that the
``state'' part of the machine can do anything that an ``ordinary
computer'' with no unbounded memory can. The tape is then just
needed to provide the unbounded storage that it is good to have
when analysing algorithms. To slightly simplify the proper and general result,
if the ``ordinary computer'' completes a run within $N$ steps it can
not possibly have touched more than $N$ distict memory locations. If
its memory is modelled by data on the tape then the most remote
bit of data ever accessed can be no more than about $N$ cells away.
Well we may aggregate raw tape cells so that some symbols are stored
spanning across say $K$ of them, but then the furthest accessed data
is only $K N$ away, and the Turing machine should snly take time
proportional to that to access it. So we find that each of the $N$ steps
of the ordinary machine gets emulated in time bounded by something of
the form $K N^2$. This quadratic overhead would of couse be calamitous in
practise, but is modest enough for a great many theoretical studies.


Rather simple extensions and generalisations to Turing Machines allow
for yet better efficiency for many problems. One can consider a
device wither with more than one tape or with just one tape but more than
one read/write head.  As a concrete example of how this makes things easier,
Merge Sort was a solid solution to sorting vast amounts of data
when computer memory was small and the data has to live on magnetic tapes.
The treatment of those tapes and those in a multi-tape Turing machine
are closely analagous to one another, so that sort of TM provides a
really solid model for analysing that sort of algorithm while avoiding
the need to worry about detailed chatacteristics of any physical
computer.

The overall message here is that if you really want toa make things
hard for yourself try to design the most compact Turing Machine
that can do the calculation you are interested in, seeing how you
can make trade-offs between the number of states, the size of the
alphabet you allow on the tape, the ugliness of how data has to be
coded for use and just how many compoutational steps will be taken
to obtain your solution. That is a wild and confusing space to
search within. A variation on this is just to look for Turing machines
that run for a seriously long time but then stop when they are started
on an empty tape. For really small machines bounds are known. For instance
if the tape can only hold one of two symbols in each call and the
machine has 2, 3, 4 or 5 states the number of steps it might take
before termination can be 6, 21, 107 and 47176870. With more states
the number of steps becomes infeasible to write using commonly-used
notations! These results show rather clearly that very small systems
can have extraordinarily complex behaviour and is systems as small
as these can behave in such extraordinary ways the detailed understanding of
larger and less artifial ones will be difficult in the extreme.

To finish this chapter it is proper to mention another variation on
these machines that has enough witty consequences that it will form the
basis of a whole sepatate chapter. Ordinary Turing machines are very
much mechanical and deterministic devices that always behave in unambiguous
and predictable ways. An entertaining variation will be the class
of machines where the state transition is not quite deterministic. In
some cases the machine will be able to choose for itself from two
different next states, with no pre-programmed or external guidance.
If the choice was made by notionally tossing a coin in each such
case you would have a randomised machine, and exploring the capabilities
there would be entertaining. But the most important case here is where
the decision between the alternate paths is made as if some good fairy
waves a magic wand and the computation proceeds such that if your
calculation could at all possibly succeed it now will. This is obviously
a delightful fantasy, and the resulting model of computation is
referred to as a ``non-deterministic Turing Machine''. It seems obvious
that machines like this could never exist in reality -- but in fact
if you abandon all concern for timings they could be emulated, and
to date nobody has been able to prove that there is no way of building
a rather efficient simulation of one.


