\chapter{Simple Pattern Matching}
Sometimes you might want to search within some text but what you want to
find is not just a fixed string. Perhaps it can allow options or
repetition of sub-parts. Perhaps you want to put some sort of wild-cards
into the pattern that is your target. There is a very well established
scheme for setting up patterns for use in cases like this, and variations on
it. Very many programming languages and even dialog-boxes in user interfaces
use at least subsets of it. So for instance the pattern \verb@*.jpeg@ may
be used to let you look for all files with the ``\verb@jpeg@'' suffix,
while at least in a Linux shell the pattern ``\verb@*.\{cpp,h\}@ will match
names that end in either \verb@.cpp@ or \verb@.h@. A fuller scheme used for
pattern matching as part of the language PERL and available through libraries
in almost all other programming languages as a bit more formal. A
pattern is built up starting with the very simplest: patterns that
consist of and match just one letter\footnote{It can also in fact be
useful to have a basic pattern that matches an empty string.}. These
simple patterns are combined using three constructions, If P and Q are
existing patterns then one can write
\begin{itemize}
\item $P Q$ -- this is a pattern that matches anything that can start with
a sequence that matches P and follows that with one that matches Q. Obviously
the very easiest use of this is that it means that you can write a sequence
of individual letters and they form a word to be spotted;
\item $P | Q$ -- here we accept anything that matches either $P$ or $Q$.
So \verb@cat|dog|rabbit@ matches strings that name creatures suitable as pets,
and \verb@p(e|a)t@ illustrates that it is sometimes useful to have
parentheses to group things. One needs some scheme to distinguish use of
\verb@(@ as a literal character to be matched or a grouping marker, just
as care is needed with the vertical bar. This pattern will match either
\verb@pet@ or how you might treat one, i.e.\ \verb@pat@.
\item $P*$ -- This is tge big one, It indicates an arbitrary repetition
of the pattern $P$. So it is in effect equivalent to $(| P | P P | P P P |
\ldots)$. Note there the initial option of no instances of $P$, i.e.\ of
this matching the empty string. A really simple instance of this would
be \verb@B(an)*a@ which matches \verb@ba, Bana, Banana, Bananana@ and so on.
\end{itemize}

There are two viewpoints that can be taken about this. One is a practical
one that adds a number of shorthand notations for things one might
frequently want to do. A particular instance of this arises because these
patterns (which are referred to as ``regular expressions'') provide
an excellent way of characterising the ways in which tokens or symbols can
be written in programming languages, and there are software tools that take
a list of patterns and create a program that splits textual input up based
on the. A first exxtension to notation that is used there is being able
to give a name to a pattern fragment and then use it later. In the programs
\verb@lex@ and \verb@flex@ one can name a fragment and then to refer to
it you put the name in braces. You also enclose literal text in your pattern
in double quotes. So for instance:
\begin{verbatim}
    digit    "0"|"1"|"2"|"3"|"4"|"5"|"6"|"7"|"8"|"8"
    number   {digit}{digit}*
\end{verbatim}
gives a pattern for any (non-empty) string of digits and calls it ``number''.
This example motivates two further expensions which clerly do not alter
the range of patterns that can be expressed but that can make the
presentation of the regular expressions concerned much more compact.
Enclosing a collection of characters in square brackets and allowing
character ranges is a help. If the opening square bracket is followed by
\^{} then the expression is treated as if was a square bracket form
enclosing all letters in your character set except the ones actually shown.
WIth this the tabluation o digits becomes just \verb@[0-9]@. The second
expension allows for the fact that \verb@*@ can indicate zero or more
uses of the pattern that preceeds it and sometimes as here you want at least
one. Replacing the \verb@*@ with \verb@+@ does that. Hence you can now write
\begin{verbatim}
    digit    [0-9]
    number   {digit}+
\end{verbatim}
Note that this could be textually expanded to the slightly clunky but
basic for for regular expressions, so the extended notations are in general
a matter of convenience rather than things that bring genuine new
capabilities. And in that spirit here are two more useful extensions that
similarly do not alter the range of patterns that can be expressed but that
may make it easier to specify them.
\begin{verbatim}
    P & Q
    ! P
\end{verbatim}
The idea is that the first of these will match every pattern provided that
both \verb@P@ and \verb@Q@ do, while the second matches any input that
\verb@P@ would fail to accept.
While it is fairly straightforward to show that adding these constructs does
not add any ability to match new sorts of pattern -- all they do is
make it easier and more flexible to specify them -- the notes here are
not going to explain the details there. Head for a suitable textbook if you
need to know exactly how it can be done! But the typical places on
your computer that provides support for regular expressions will typically not
support these last two because in fact they unlock levels of practical
difficulty that are hard to comprehend!

Thus far the use of regular expressions to provide patterns that you can
try to match against input text seems really rather easy despite the
pessimistic statement above. So here to give some insight into just
why adding those two last capabilities is so bad let's state what
problem it turns from tolerable into being solvable in principle and
theory but utterly dreadfil in practise.

Consider a regular expression built up as follows and over an alphabet
that consists of just the two letters \verb@a@ and \verb@b@.
If one sets up a fairly messy example of such an expression it might
not be instantly obvious what inputs it will match. The question that gets
tricky is ``Is there any input at all that it will {\bf not} match?''.
This may seem a frivolous and artificial question, because what on earth
would be the point of a pattern that matched absolutely all input? Well
actually it is not as stupid as it at first sounds! The components used
to build up the regular expressiion may be modelling the behaviour of
some machine or the progress through some program as it inspects the
input. Succeeding in making match could correspond to successful
processing, and so input that is not matched could amount to input
data that causes the machine or program to fail. If that sounds a bit
fanciful it is in fact almost exactly the insight used when considering
how hard it will be to answer our question! And regular expressions and
a certain simple class of machines or programs are very closely linked.

There are two major parts to understanding where the difficulty comes
from. The first is to show the way in which a regular expression can be
``compiled'' into a nice program that will check for things that match it.
The way of doing this in effect builds a sort of flowchart for the
program, but it is not quite the normal sort. This one consists of
blobs referred to as states and arrows fromn one to another where the arrows
are laballed with characters from the input alphabet. One state is
identified as where one starts, and any of the states can be marked
with a ``success'' tag. The concept is that from where it starts the arrows
are followed and when as a result one is positioned in a success state
that means that the text absorbed to date has been matched. So for instance
one of the earlier very easy cases ends up as
\begin{verbatim}
                   /-a->\ 
   (start)  -p-> (.)    (.) -t-> (success)
                   \-e->/
\end{verbatim}
which should be very easy to follow and to understand how it relates to
the original regular expression.

Expressions involving the \verb@*@ operator introduce loops in a
rather straightforward way. The harder issue thart arises in the
``\verb@Bananananana@'' example is that one ends up with a state where
two arrows emerge from it each corresponding to the same symbol.
% B(an)*a. This clearly deserves a proper picture not this dire
% ascii art!
\begin{verbatim}
                  /-a-> (success)
   (start)  -B-> (.)
                / \-a-> (.) --|
                ^             v
                \----<--- n --|
\end{verbatim}
This is referred to as being nondeterministic in that there is no
clear way to know which \verb@a@ arrow to follow. The understanding is
that the flowchart is deemed to recognise a string if at least one way
of making those arbitrary-seeming transition leads to it ending up
in a success node at the end of the input.

There are tricks that arrange that if one has a flowcharts corresponding to
two regular expressions $P$ and $Q$ that one can construct ones for
$P Q$, $P | Q$, $P \& Q$ and $P*$. The one for $! P$ is especially
easy since all it has to do is swap which states are or are not
tagged as success. Along the way these tricks can expand a nondeterministic
scheme into one where each choice is unambiguous. As before the
fine details of this are skipped here because none of it is difficult
enough to match the book's title.

An important observation is that the flowchart you end up with has
a finite number of nodes, and if there is an input that can be presented
that is not going to be matched that means there must be a non-success
node present and a path from the starting state to it. Viewing the flowchart
as a sort of maze and checking for this is very much something that
will not correspond to a hard program at least provided you are not
worried too much about performance. So phrased the way we need it
here we claim that the question as to whether there are any strings
not matched by a given regular expression is one where it is
possible to write a program that answers it in a finite amount of
time. To be specific, the program starts by elaborating the regular
expression into a flowchart and it then maze-searches that to see if
a non-succes state can be reached from the start position.

Well the normal language used for this would be to say that the
question is ``decidable'' and can be resolved by mapping the
regular expression onto a ``deterministic finite state machine''\footnote{
Well a non-determistic one might be as good, ane different authors
us different names such as ``finite state machine'' or they
abbreviate things to terms such as ``NDA'' for non-deterministic
accepter. But the concept being discussed is the same whatever language
is used when discussing it}
and checking if a ``non-accepting state'' is reachable from the start.

At this stage it may be useful to point out that a deterministic flowchart
maps onto a really simple computer program that has a simple loop and
a look-up table:
\begin{verbatim}
   state = start_state
   while more_input do
      state = transition_table[state, next_input_character]
   return result_table[state]
\end{verbatim}
and this fact is part of why this sort of pattern matching is so
popular in software.

This has set up a problem that we can guarantee to solve, and despite the fact
that setting up the \verb@transition_table@ as used above involves a bit
of effort\footnote{\ldots and that good implementations seek ways of
representing it in more compact ways than just a simple big rectabgular
block} it really does not look too bad. But now we can lead into the
pay-off in terms of difficulty. By designing a regular expression
cunningly and using all the extended notations we have it is possible
to arrange that if the regular expression is made up of $K$ symbols
then the size of the flowchart and hence the time and effort needed
to answer the initial question can end up greater than
$2^{2^{\ldots 2^2}}$ where the tower of exponentiation has height $K$.
This easily gets way beyond astronomical, as can be confirmed by reviewing
the numbers that represent the side of the galaxy or known universe. On
that basis it is probably fair to declare that obtaining the answer is
difficult, even though one can have written a program that would
deliver it if it could ever run to completion. And that it is absolutely
known that the program would complete if give long enough -- the only
problem is that ``long enough'' might exceed the lifetime of the
universe.

The assertion that something is that hard deserves some justification, and
so here is a sketch of how it arises. All sorts of awkward details will
be skimmed over!
 
