\chapter{If you See What I Mean}
\begin{quotation}\textit{
Elsewhere you will find attempts to get to the root of what computing is
really about by focussing on either data storage (Turing machines) or
the flow of control as you perform a computation (Counting machines). This
chapter can be seen as yat anther way to look at things by considering
``functions'' as primitive. It lends itself to presentation in the form
of reasonably accessible (family of) programming languages where the
most limited of those has nothing except functions to work with but
higher up varieties can be genuinly practical and convenient to work with.
}\end{quotation}
Turing machines provide a model for computation that (perhaps unexpectedly)
can give really good insight into costs. But setting up programs for them
is not really anything like conventional programming and they do not
adapt well to day to day use. Minsky or Counter machines have their
behaviour expressed by flowcharts and come much closer to being amenable
to casual use, but they exhibit totally absurd run-times that render them
quite unsuitable for anything other than theoretical purposes. That leaves
a gap in the market to be filled by a formal model of computation that can
be reasonably easily adapted for use as a practical programming system. A
key candidate for this is called ``lambda calculus''.

The fundamental idea here ia that functions should be seen as the basic
building block for everything else. When first introduced this was accepted
as a neat theoretical idea and it led to what I will impolitely describe
as niche programming languages, but over recent years it has been recognised
as such a good idea that basically all modern programming languages
incorporate features that are directly derived from it. A {\em lambda
expression} denotes a function and is seen as freestanding entity. For
instance in earlier usage one could only introduce a function by a notation
that gave it a name, as in
\begin{verbatim}
    f(x) = 3*x + 1
\end{verbatim}
as a definition of a function called \verb@f@. With lambda calculus the
name f is not needed and we can write just this function as \\
\verb@    @$\lambda$\verb@ x . 3*x + 1@\\
where the name just after the $\lambda$ indicates what the formal argument is
and the section after the dot is the body of the function. About the only
action that can apply to a lambda expression is that it can be applied to
an argument, and the consequence of that is just about the same as when the
previous function called \verb@f@ is applies to an argument. It is almost
as simple as that!

Well there are two issues that need discussion that arise when several lambda
expressions arise together. The first rule is just to give a pedantic
confirmation that things happen the way you would naturally hope, and
it insists that when the body of a function has a value for the function's
formal parameter substituted in nothing improper happens. One version of
it starts by noting that in any lambda expression one could rename the
variable that is bound to anything else and so the example shown above is to
be viewed as entirely equivalent to \\
\verb@    @$\lambda$\verb@ y . 3*y + 1@\\
and the mapping from one text string to the other is known as alpha conversion.
Now if you applied out first lambda expression to an argument that was the
literal symbol \verb@x@ rather than a number there would be a potential
source of confusion about substituting for \verb@x@ in the body. All
possibiity of uncertainty can be overcome if one used alpha conversion on the
function just before applying it to make its formal parameter a new symbol
that could not conflict with anything. Another part of what is essentially the
same issue arises when the body of a lambda expression includes further nested
lambdas and variable names are reused. It is proper to apply scoping rules so
that inner lambda bindings take priority over outer ones, but even here use
of enthusiastic alpha conversion can keep all names separate and avoid
risk.

The second issue is that if one has a reasonably large nest of lambdas (and
some examples will arise later) it could be that there are several places in
the whole where a lambda could be applied to an argument. When there are
several options for the order to do these operations does it matter which
is selected and if so what is the proper strategy? On this there is some
divergence of opinion! From a theoretical point of view it is considered
proper that where their is a choice one should select the leftmost
outermost application. This is known as normal order reduction and if there is
any chance at all of continued application of lambda expressions to
arguments ever terminating with a form where there are not more available
than this guarantees to reach that state. The alternative is that
innermost lambdas are processed first. This can sometimes risk
unnecessary exploding or looping sequnces of operations, but is generally seen
as easier and cheaper to implement. So many but not all programming
lamngauges with lambda support follow the second path.

It is perfectly feasible to write out everything that is to be discussed in
this chapter using loads of $\lambda$ symbols, But for many people it
will be easier to work with a notation that feels more familiar (but that
can be expanded directly into the raw lambdas). A paper by P. J.
Landin\cite{next700} has as its title ``The Next 700 Programming Languages''
and introduced \verb@ISWIM@ (If You See What I Mean) as a sketch, and
so rather than latching onto any particular real or modern notation that
exposition used here will be based on that.
   
ISWIM starts by using a keyword \textbf{where} to give names to things.
To be a little pedantic here are a couple of examples and how
that would expand onto raw lambda expressions:
\begin{alltt}
 3*x+2 {\bf where} x = a + b       : (\(\lambda\) x . 3*x+2) (a + b)
 f(a+b) {\bf where} f(x) = 3*x + 2 : (\(\lambda\) f . f(a + b)) (\(\lambda\) x . 3*x+2)
\end{alltt}

The hope here is obviously that the ISWIM versions are un-threatening
and so will make things easy to understand. As well as \textbf{where}
that puts a definition after the use of something it will also be
possible to use \textbf{let} to put a definition first, and here we will
often use that with the intent that the definition provided should be
available to all subsequent input. What is more likely to cause some
concern to start with is that in this world functions are first class
citizens with all the rights and privileges entailed. In particular they
can be passed as arguments and returned as values. A major lesson that
lambda-calculus teaches is that this is liberating an powerful and
so it is used a very great deal.

The ISWIM examples above use function definitions and \textbf{where}
clauses to provide the structure of a program, and then have numbers
and arithmetic operations. As a prototype for 700 variant languages ISWIM
can imagine a wide variety of built-in data types and functions that
can apply to them, so that it can be tuned for particular application areas.
I will allow for the creation of user-specified data types. But it tends
to de-emphasise sequential programming, with its documentation noting that
in Algol 60 it was already possible to avoid that quite a lot by virtue
of allowing conditionals as expressions as well as statements. It can view
a sequence of several values as in \verb@A, B, C@ to be viewed as a single
item, and that item could be returned as the value of a conditional
expression or passed to provide three arguments for a function that needed
that many. The variant or variants of it used here will be ones tuned to be
good for describing things that can be done with lambda expressions!

From the tiny examples shown above it is possible to push explanations either
up or down in terms of the level of abstraction involved. Here we will first
show that lambda calculus using nothing more than function definitions and
even restricting those to the one-argument case is in fact capable of
modelling all computation. And that amazingly the steps it needs to take to
do this are all remarkably concise and may even not be terribly inefficient.
After that it will be explained how lambda expressions can act as a
convenient and well defined intermediate stage to be used by compilers while
mapping from source code down to some machine-specific level. And When that
has been done it is easy to observe that any programs in the languages that
those compilers support can be rendered as lambda expressions. A final
section will comment on a somewhat archaic but thoroughtly practical
language that was explicitly built to illustrate this but that has been
used to build large and complicated programs.

To build up an understanding of computation from lambda calculus we
start by noting that a lambda expression only has a single argument. In
much of what follows it is going to be convenient to think in terms of
functions with two or more arguments. These can be rendered using a
technique called ``currying'' after H. B. Curry\cite{curry} who popularised
it. We will write function definitions as if they used several arguments
as in
\begin{alltt}
   {\bf let} f a b c = a+b+c+2
\end{alltt}
using spaces between the multiple parameters rather than enclosing the
collection of them in parentheses.
We write in parentheses to show how we intend or expect to view this
as grouped. In this case both the description of the pattern for the
function (i.e.\ the bit to the left of the \verb@=@) and the additions in
the body associate to the left.
Then there is a bit of rather dubious renaming and a use of \textbf(where)
and tha name \verb@F@ that is really not quite proper but that turns the
main definition into one with a single argument which is then easy to
transform into a lambda.
\begin{alltt}
   {\bf let} ((f a) b) c = ((a+b)+c)+2
\(\Longrightarrow\)
   {\bf let} F c = a+b+c+2
   {\bf where} F = (f a) b
\(\Longrightarrow\)
   {\bf let} (f a) b = \(\lambda\) c . a+b+c+2
\(\Longrightarrow\)
   {\bf let} f = \(\lambda\) a . \(\lambda\) b . \(\lambda\) c . a+c+c+2
\end{alltt}
Even if the last line might look slightly awkward to start with it
shows that the ISWIM-style notation that defines a function that in a
certain sense has three arguments does correspond to respecable
lambda calculus.

Given that the first thing to be done is to show how easy it is to
provide truth values (i.e.\ {\em true} annd {\em false}) and a
conditional that tests them. Annd instantly from that {\bf and},
{\bf or} and {\bf not}:
\begin{alltt}
   let true a b = a
   let false a b = b
   let if p X Y = p X Y
   let and a b = a b false
   let or a b = a true b
   let not a = a false true
\end{alltt}
These can also look a bit strange to start with, but observe how very
concise they all are and then just verify them by writing down
combinations of them and using the funcction definitions as rules to
check how things behave. However there is one big caveat in all this --
it only works nicely provided you use normal order reduction, because
that manages to arrange that for instance when you write \verb@if p X Y@
that the evaluation of \verb@X@ and \verb@Y@ gets deferred until it can
be determined that only one actually needs to be processed. With
the evaluation strategy that deals with innermost expressions first
both \verb@X@ and \verb@Y@ would get expanded there and often that
would lead to a loop or other disaster.

Note that for the above to behave the value \verb@p@ tested by \verb@if@
must evaluate to either \verb@true@ or \verb@false@. In a bit there will
be a discussion about how that might be enforced.

Given truth values and conditionals the next feature of ``real programming''
to consider will be data structures.  A useful building block from which
pretty well anything else can be constructed will be the ``ordered pair'',
where we will have a function \verb@pair@ that creates on and then
\verb@left@ and \verb@right@ that retrieve one or other of the
two included items. Again the lambda forms that implement this are
amazingly concise and you just have to try following through what
happens when you use them to convince yourself that for instance
\verb@left (pair A B)@ will recover \verb@A@:
\begin{alltt}
   let pair A B f = f A B
   let left P = P true
   let right P = P false
\end{alltt}

There are two things that can be done instantly once you have pairs. The
first is to use clusters of them to build triples, 4-tuples and
larger structures with any fixed number of components. The second is
to pair an object with a tag (which might be a truth value) so that
as a while it can have two (or more) possible shapes. As an
illustration of this consider a way of handling lists that can have
and number of elements (including zero):
\begin{alltt}
   let emptylist = pair false <anything>
   let node A B = pair true (pair A B)
   let isempty L = not left P
   let first L = if (isempty L) <error case> (left (right L))
   let rest L = if (isempty L) <error case> (right (right L))
\end{alltt}
which exposes the fact that dynamic structures like this have
to carry extra information so that it is possible to detect when
the end of a list is reached. A similar scheme could describe trees.
The sections in the above that are written in angle brackets are
parts that ought never to be used so anybody confident that their
code would not contain any errors could use absolutely anything
to fill those gaps.

Next comes a representation of numbers and arithmetic. Well one
scheme that could handle positive integers tolerably well would be to
express them all in binary and use a list of the bits, with \verb@true@
used for \verb@1@ and \verb@false@ for \verb@0@. With that representation
arithmetic operations would have costs driven by the bit-length of
the numbers, and really that is a proper cost even though we tand to
be used to thinking of arithmetric as having unit cost per operation. This
modelling comes much closer to revealing how many electronic components
will be activated in performing an operation.

But coding up binary arithmetic is a bit tedious, so it is fun to show that
it is seriously easy to make pure lambda calculus explain the behaviour
of integer arithmetic\footnote{To be more pedantic natural number
arithmetic, i.e.\ only using positive values} really neatly.
Rather than thinking in terms of zero, one, two, three and so on it
is good here to thing in terms of never, once, twice, thrice and the like,
and a ``number'' can then be a function that applies a second function
multiple times:
\begin{alltt}
   let never f a = a
   let once f a = f a
   let twice f a = f (f a)
   let thrice f a = f (f (f x))
\end{alltt}
and this pettern allws you to have a function corresponding to any
number of copies of \verb@f@ you could want to set up. With this
representation of numbers a scheme to do basic arithmetic is almost
frightendingly simple!
\begin{alltt}
   let increment n f a = n f (f a)
   let add m n f a = m f (n f a)
   let multiply m n f a = m (n f) a
   let power m n f a = n m f a
\end{alltt}
and also a test for zero is remarkably easy to arrange
\begin{alltt}
   let iszero n = n f true where f x = false
\end{alltt}

Again one can appreciate how compact all the above are even without (at first)
understanding how they work. And to get insight into the working it is
possible to start with simple expressions such as \verb@power twice thrice f x@
and follow through the definitions to observe it expanding until it ends up
as (in this case) \verb@f(f(f(f(f(f(f(f(a))))))))@ which used \verb@f@
$8 = 2^{2}$ times. Or \verb@iszero twice = twice f true = f(f true) = false@

There is one thing to be done here that is slightly more tricky, and one
issue that has so far been swept under the carpet. The tricky issue is
a function \verb@decrement@ to find the predecessor of a n. It should be
expected that this will not be quite so straightforward because \verb@never@
does not have a proper predecessor. There are however several ways of
managing this. The one shown here is not the standard one but is perhaps the
most compact -- but it relies on the precise way in which \verb@increment@
has been coded and it achieves its result by handing that an argument that
is rather improperly not a number but that has been chosen so that the
resuly returned is \verb@never@. See how that can be done, starting with
a letter X standing for what we will invent:
\begin{alltt}
  increment X f a = never
  increment X f a = X f (f a) = never
  let X a b = never
\end{alltt}
and the three lines above that intoduce a function \verb@X@ that
in just this one contexe serves as $-1$ we end up with an implementation
\begin{alltt}
  let decrement n f a = n f X a where X a b = never 
\end{alltt}
Concise it possibly confusing\footnote{
A scheme that does not cheat by passing things to the increment function that
are not numbers works by having a function that accepts an ordered pair as
its argument: \texttt{let inc (a,b) = (a+1,a)}. If this is applied $n$ times
to a starting value (0,0) the result is ($n$,$n-1$) so the second component is
the predecessor of $n$}

Any reasonable programmer will see that given decrement it will now be
pretty trivial to code up subtraction and then division. If negative numbers
are needed that can be handled using a sign-and magnitude representation
by using \verb@pair@ to glue an explicit sign only every number.

Now for the issue that had been avoided that far (but when you look back
at all the fragments of code to date it does not impact them). In raw
lambda calculus the functions do not have names. That means there is no
obvious way for the definition of a function to refer back to the function
itself. In ISWIM you could have felt tempted to write things rather like
\begin{alltt}
  let factorial n = if n=1 then 1 else n*factorial(n-1)
\end{alltt}
as a simple recursive definition. On expanding that to lambda
calculus the use of the word \verb@factorial@ within the body
of the function will be improper and it certainly has no simple
way to relate to the name you happen to be using for the function.
So it looks as if recursion is going to be a problem! Happily this
can be resolved. Often the way of sorting it is thought of as a bit
strange. The presentation here is based on thinking of recursive
definitions as just a notation for programs of infinite length
that unwind it:
\begin{alltt}
  let f1 n = if n=1 then 1 else n*f2(n-1)
    where f2 n = if n=1 then 1 else n*f3(n-1)
      where f3 n = if n=1 then 1 else n*f4(n-1)
        where f4 n = if n=1 then 1 else n*f5(n-1)
          ...
\end{alltt}
and provided that chain of nested definitions is continued to
a depth that is at least as great as the value of an argument $n$ we are
about to use this will do just what is needed. This sort of expansion
can be applied to any recursively defined function\footnote{
ISWIM insists on the use of \texttt{let rec} when a recursive function is
being defined to highlight that something special is going on.}.

The magic that makes it possible to create unboundedly nested things in
raw lambda calculus is a function traditionally called \verb@Y@. Its
behaviour can be characterised with the identity
\begin{alltt}
   Y f = f (Y f) = f(f(f(...
\end{alltt}
but because this is an explicitly recursive presentation it can not be
used directly. However there are many ways in which ISWIM or lambda
calculus can provide a \verb@Y@ function without needing recursion,
and the simplest is
\begin{alltt}
  let Y f = g g where g h = f (h h)
\end{alltt}
In ISWIM notation this is really concise and rather easy to understand!
Just simply following the steps we have \verb@Y f = g g = f (g g) = f (Y f)@.
ands the ISWIM definition did not mention \verb@Y@ within the right hand
size of its definition so it translated into 100\% valid lambda calculus.

The general use of \verb@Y@ is that recursive function definitions can be
expanded using the following rule:
\begin{alltt}
  let f x = ... f x' ...
\(\Longrightarrow\)
  let f = Y G
    where G f =
      F where F x = ... f x' ...
\end{alltt}
and the key thing here is that the recursive reference to \verb@f@ now
has a meaning because it is talking about the argument to G, and if you
trace through expanding the definitions out a few times you can see that
this ends up being exactly what \verb@f@ was to end up as.

There really only one further issue that needs to be addressed -- having
multiple functions which are mutually recursive. In fact this is
rather easy to cope with. It will be sufficiant to explain what to do
with just two such, since larger numbers can just use the same technology
``written large''.

If there are a two functions say \verb@f@ and \verb@g@ echo of which
depends on the other one just considers putting them as the
two components of a \verb@pair@. Then the definition in ISWIM form
might look something like
\begin{alltt}
  let P = (pair F G) where f = first P and g = second P
\end{alltt}
where \verb@F@ and \verb@G@ are the two definitions written as
lambda expressions so that there are no indications of arguments on the
left hand side. This is now merely a definition of P in terms of itself
and the expansion to use \verb@Y@ shown before can be used to turn it
into a respactable lambda form. 

We now have boolean logic, data structures, arithmetic and recursion and
with those it is pretty easy to code up anything else that can be
imagined\footnote{Well interactive input and output, file-system access and
the like are not parts of the world of concern just here!}. And the
mapppings from them down onto the extraordinarily simple basic rules
of lambda calculus are concise and in general not too unpleasant to work with,
so if anybody {\em really} wants to reason about the behaviour of code
starting from first principles it can be reasonable to select lambda
expressions and their behaviour as those fundamental principles. But
now numbers and all the rest can (when necessary) be explained in this
rather gory manner it is perhaps reasonable to consider a version of
ISWIM that has all such features as built in primitives that can be
implemented using traditional computer hardware and reasoned about at their
own leval of abstraction. The fundamentalist view of lambda calculus then
just sits in the background providing assurance that the foundations for
all that is built are really secure.

Even if some of the code fragments above look unusual it will be easy to
see that all are rather concise and none hide particularly terrible
costs. Well modelling the integers in this simple way will make them slow
but for reasoning about things and proofs that is not a real concern, while
for realistic execution of code one could either use lists that provided
binary arithmetic or more plausibly build in genuine integers as a primitive
type. But the gap between a really fundamental and primitive model
(i.e.\ lambda calculus with nothiing beyond functions to work with) and
a world in which computing could be realistic is rather small.

Well having looked downwards from practical computing to see how it maps
onto a nice theoretical model the next thing to do is to look sideways. This
will be a consideration of the use of lambda expressions as part of
a compiler. Specifically the idea is to use lambda expressions or at least
something very much like them as an intermediate code that the user's
original programs are coverted into before things get mapped down as
far as machine code. Having a clean intermediate form is generally valuable
because it is liable to be simpler to analyse then the original
complicated language things started off in, and transformations and
optimisations done there can be independent of the precise machine that
will eventually be targetted.

For the discussion here it will again be convenient to use a version
of ISWIM notation to show the lambda-calculus stuff. It will not make
sense to cover every single part of the expansion from C++ or any other
high level language\footnote{Some people would object to calling C++
high level.} but representative important challenges can be covered.

The first thing to note is that this far ISWIM has not included assignment
statments or \verb@while@ loops -- far less \verb@goto@ statements. But
a ``real'' programming language might! A feature of such a language will
be that programs are made up of sequences of statements that are usually
obeyed one after the other. If one is using the lambda calculus as an
intermediate representation for code within a compiler than it will not
matter too much of that version looks a bit ugly! Given that attitude
we can take each line of the original user code and make it into
a function all on its own. All the variables the user has will be passed
as arguments to this. The function we have just invokes the one
that corresponds to the following line with arguments adjusted to
capture changes made to the values of variable.
Suppose we had some C++ code that read
\begin{alltt}
  <start here>
  int r = 0;
  for (i=0; i<10; i++)
    r = r + i;
  <end here>
\end{alltt}
this might expand into something along the lines of
\begin{alltt}
  let start () = f1 0   % call f1 with r=0
  let f1 r = f2 r 0     % introduce i starting at 0
  let f2 r i = if i < 10 then f3 r i else f4 r
  let f3 r i = f2 (r+i) (i+1) % the loop body
  let f4 r = ...  
\end{alltt}
This is perhaps suprisingly not that dramatically bulkier that the
code that it started with, but being in lambda calculus is in a form
where reasoning about it and transforming it is liable to be safer than
trying to work with the original.

Variations on the above cope (fairly) gracefully with many local features
of programs. Function calls (and perhaps especially the interaction between
them and exception handling raise further challenges. Before explaining how
lambda calculus can help it will be useful to review how compilers tend to
map procedure calls onto sequences machine code instructions.

I many cases arguments to a procedure will be passed in machine registers,
so to call a function the compiler will arrange to have argument
values loaded into the registers that they will belong in. It then needs
to do the procedure call itself. That involves saving a return address
somewhere and then jumping to the procedure entrypoint. The code for the
procedure they knows where to find its arguments. It does what it needs to
and puts its result into another agreed register. It then recovers the
return address that it had been provided with and jumps to the instruction
that is identified. This explanation has skipped discussion of stacks and
of arranging that the procedure does not corrupt registers or data that
the caller needed to have preserved, but that needs to be arranged as well.

Well an insight that emerged from lambda-calculus enthusiasts was that the
return address could be viewed as nothing different from just an additional
argument. It is a value put in a register before the procedure is entered.
And similary the return step looks very much the same as the process of
viewing that return value as the entry point of a function and calling
it giving it an argument that was your ``return value''. Thiw view
feels fairly natural if you are ready to accept functions as first class
objects rather than as something seriously different from numbers and
arrays. So now we explain the code by saying that to call a function
you enter it having placed in agreed locations all of its arguments
together with a function that you call a ``continuation'' (and that
you used to call a ``return address'' and view as something much more special).
Then when the function has done its stuff it just invokes its continuation.
Well an odd consequence of this is that nothing ever truly returns --
all code works by chaining from one continuation to another, but given that
this is merely a semantically clean way of describing what was going
to happen anyway there is not need for it to be expensive. And because
it means that argument registers and return address ones are not now
treated as separate sorts of thing it may simplify optimisation.

A further great joy of this is that it provides a way to explain
exceptions and \verb@catch/throw@ behaviour. A \verb@catch@ just sets
up a continuation that would proceed into the part of its code that
responds to an exception. This is put somewhere same or passed along
to functions that are called. If a \verb@thow@ has to be performed all
that happens is that this alternative continuation is triggered and the
one that would normally have applied just gets ignored. What a neat and
elegant way to explain something that previously might have seemed
a bit messy!

The final section in this chapter is an explanation that the
roots and the legacy of ISWIM and of lambda calculus permeate
quite a lot of the world of computers at the practical level.
Historically the language LISP\cite{lisp} had lambda calculus as one
of its explicit inspirations, and over the years many serious bodies of
code were implemented in it. Those includes the operating systems for
special purpose computers that were in their day prime ones for development
of artificial intelligence code and packages that can perform symbolic
algebra in ways that can help students, scientists and engineers. More
recently SML\cite{SML} (used to develop heavy duty theorem provers),
Haskell\cite{Haskell} (a language that features normal order reduction)
and OCaml\cite{and ``industrial-strength'' variant on the concept} can
all be seen as having lambda calculus at their core and as being
among the 700 languages that the ISWIM paper talked about. So this is
an topic that spans from foundational theory all the way up to
large scale practical application, but with the gulf between those two
much less than is observed with other theoretical models.

