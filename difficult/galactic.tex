\chapter{Galactic algorithms}

Gaalactic algorithms are ones that have costs that grow slowly with the
size of their input to an extent that for large enough cases thay will
do better than simpler methods, but that suffer from such large
overheads that they will never become practically useful for any
plausible cases. The finest ones will provably achieve the slowest
possible growth rate for the problem that they solve. Some of the
most impressive have overheads that can be expressed as a constant factor
multiplier in their cost formula with that constant exceeding astronomical
proportions. There will obviously be case that come close to fitting into
this category. Because such methods are not of a lot of practical use one
cound be tempted to view them as frivolous, but where they represent
a better growth rate than any other known method or are provable optimal
in that respect they can both be of intellectual interest and can provide
ideas to guide work towards schemes that can be useful in the real world.

A number of galactic algorithms illustrate how much more complicated
things get if you aime for the absolutely best possible method, but
here we start off explaining one that has been known for some time
and is neither terribly complicated nor terribly expensive, but that
despite it giving a theoretical speedup will onlt achieve that in cases
pushing against the size limits of the biggest available computers, and
even then its advantage will be slight.

\section{Matrix Multiplication}
The straightforward scheme for multiplying a pair of square matrices of
size $n \times n$ has cost that grows proportional to $n^{3}$. Our first
galactic algorithm represented a breakthrough discovery that this
growth rate was not the best possible, and that a tolerably straightforward
scheme could achive a cost that grows as $n^{\log_{2} 7}$ which is
about $n^{2.80735}$. To show how this is done it is sufficient to
consider the very simple case of forming the prodict of a pair of $2 \times 2$
matrices $A$ and $B$. The four elements of their product as evaluated
in the classical manner aee just $A_{11} B_{11} + A_{12} B_{21}$ and three
rather similar looking values and a total of 8 multiplications and 4
additions are used. A paper by Strassen\cite{strassen} instead managed to
show that using just 7 multiplcations that worked on carefully chosen sums
and differences (e.g.$($A_{11} + A_{22})(B_{11} + B_{22})$) it was possible
to express all the values needed in the product matrices as sums and
differences involving these 7 values. Here we leave finding exactly how
to do that as either a puzzle to work on or a topic to research! A variety
of different schemes have been found, all ending up with just 7
multiplications but some with more and some with fewer additions.

Once the base case of $2 \times 2$ matrices has been handled large case can
be treated by partitioning each matrix into four blocks (padding out with
zeros if necessary to make those blocks all square and all the same size)
and coping there by using just 7 multiplications (which are now matrix ones)
on those half size blocks. An analysis of the growth rate in time spent
using this scheme fairly clearly suggests that doubling the side of
the matrices multiplies the amount of work by a factor of 7, hance the
$n^{\log_{2} 7$ result. This is clever and quite elegant, and not even
too hard to program. But as against the classical method it struggles
even to break even until matrices are larger than almost all applications,
amd it certainly does not deliver truly valuable speedup for some way
beyond that. A further concern with this approach is its impact on how
rounding errors propagate through the calculation.

But this is not the end. Since the Strassen work others have sought
to find schemes that are at lease assymptotically faster, with a long
sequence of reports og gradually reduction of the exponent in the
cost function. As of 2026 the best growth rate that has been reported is
around $n^{2.37134}$ which looks like a distinct improvement over
Strassen. So far nobody has been able to show what the ultimate limit
for solving this problem will be. Most of these improved schemes work
by splitting large matrices into more then 4 blocks and all of them
are even further from practicality then the Strassen method.

\section{Integer multiplication}
Multiplying integers is a pretty fundamental operation, but here to
make things harder the emphasis will be on large integers -- typically
ones with from hundreds to millions of decimal digits. Things start
off in a style almost parallel to the matrix multiplication one,
with first the simple ``schoolbook'' methos that will form the
product of a pair of $N$ digit numbers using around $N^2$ operations.
The first improvement on this is dus to Karatsuba\cits{karatsuba} and
works by observing that for 2-digit numbers one can form the
partial products $a_{1}b_{1}, a_{1}b_{2} + a_{2}b_{1} and a_{2}b_{2)
using 3 rather than the obvious 4 multiplications by calculating
$a_{1}b_{1}, (a_{1}+a_{2})(b_{1}+b_{2}) and a_{2}b_{2} and doing a couple
of subtractions. By taking large numbers are splitting each in half
this can be applied recursively leading to a method where costs
grow like $N^{\log_{3} 2$. This scheme can become proper to
use when numbers exceed say 10^{300}, but the exact break even point will
be quite sensitive to details of the computer used and just how everything
was coded. So this not a galactic algorithm.

The next scheme of note may start to be competitive when multiplying
numbers of magnitude around 10^{5000}. The exact point where it becomes
realistic can vary quite a lot and 5000 digits is perhaps better
characterised as stellar rather than galactic, so this is a method that
is at least relevent in some slightly specialist cases. It is however
much harder to explain in elementary terms, and so what will be presented
here is an overview perhaps uses enough technical language to show
that it starts to count as ``difficult'' or at the very least ``reasonably
advanced''. So for those who do not recognise the words used here this
will just show how much messy technology a simple-seeming task such as
integer multiplication can involve. For those who then want to follow
through by re4ading textbooks or scouring the web it may give a broad
overview so they know how the things they read up on fot together. And
those who already understand this well can just smugly skip to the next
section.

Sch\"{o}nhage-Strassen works by first padding its integer inputs so that
each have a suitable power of two bits. At least as an abstract algorithm
if expresses its operations in terms of bits not digits so it is kept
fully honest and does not hide costs within arithmetic on digit-sized
units. Then if the numbers are of length $N$ it clumps the bits into
clumps of size around $\sqrt N$ and views each clump as a digit, and so
there are around $\sqrt N$ digits. Well there can be some pedantry about the
exact size of digits that is needed but the decomposition as described
here is close enough for use in this overview.

The next issue is that it views each collection of digits as a vector
and it takes a Fourier Transform of it. So what is a Fourier Transform?
Well it amounts to multiplying a vector of length $K$ by a special form
of matrix where the entries in the matrix are all powers of a value $\omega$
where $\omega^{K} = 1$ but no lower power of $\omega$ has that value. In
the normal world one would say that $\omega$ is a complex $K$th root of unity,
and that it is primitive, so at first it looks as if complex numbers are
involved. However to avoid them (and any risk of creeping rounding error
that could hurt if floating point arithmetic was used), Sch\"{o}nhage and
Strassen used modular arithmeic. In other words they have some number $P$
so that after any operation on integers they just retain the remainder
when the natural result is divided by $P$. A big part of their cleverness
is that by choosing $P = 2^{M}+1$ for a suitable $M$ two wonderful things
can be achieved. First it becomes possible to use 2 as the value for $\omega$
and that means that multiplication by powers of $\omega$ can be mechanised
as simple cheap shift operations. Secondly the remaindering operation
where a double length integer is to be reduced modulo $P$ can be performed
by just subtracting the top half of the number from its bottom half.

One interpretation of what a Fourier Transform does is that it views
its input vector as the corfficients in a polynomial and evaluates that
polynomial at all the powers of $\omega$. If that is done to two polynomials
and the resulting values are pairwise multiplied the result will be
the values of the product polynomial at all those points. An inverse
Fourier Transformation will act as interpolation and recovers the
coefficients of the product polynomial. This can be re-interpreted
as an integer following some simple carry operations. A further key
feature of Fourier Transforms is that on using vectors of size $K$ can
be completed using around $K \log K$ arithmetic operations. Of course
here each of those arithmetic operations is using modular arithmetic
modulo $P$ where $P$ is a pretty huge multi-digit number -- but because
of all the powers of 3 involved that is not in fact seriously painful. In
fact the worst part of all of this is the pairwise multiplication of elements
of the transformed vectors and that generally needs to recurse into a
further layer of use of the whole procedure -- at least until things get
small enough that classical methods will win.

The ``polynomial product'' produced by the Fourier Transform scheme has
been computed modulo $P$ but we want exact and correct integer results.
Well provided $P$ was greater than any value that could legitimately
arise in that product all is well and the calculated values will be
the perfect integer results as needed.

The above explanation know it is a sketch. It has omitted the need to
pad various things up to powers of 2 and the proper care about splitting the
inputs up so that the value of $P$ used is big enough to ensure that
results are correct. Those details of course matter in a proper formal
explanation of the algorithm and in any implementation of it, but perhaps
they repesent details best defered to a second reading about the method.

The Sch\"{o}nhage-Strassen scheme for multiplying $N$ bit numbers has a
cost that grows proportional to $N \log N \log \log N$. Because it is
concerned with truly gigantic input numbers (i.e.\ values of $N$) it
becomes proper to worry that if digits making of numbers are stored
in computer arrays of length $N$ then index arithmetic used to work out
which digit to touch next can become beyond th4e scope of single
computer operations. For instance a modern 64-bit computer will not
cope trivially with arrays with more than $2^{64}$ elements! So the proper
analysis of costs here needs to be done in terms of some suitable abstract
machine that does not have any limita at all on the bulk of data it
can store and maniplulate. Variations on Turing machines are commonly
used, and amazingly some of them can deliver the cost growth rate
noted above.

This scheme, however messy, is still used in some practical applications.
But it is not the end of the road. A succession of authors have described
yet more elaborate schemes that reduce the $\log \log N$ term in the
cost function, culiminating one\citeHarveyvanderHoeven} where the cost
is precisely proportional to $B \log N$. This is a great theoretical result
in that it is conjectured (but not yet proved) that this is the best
growth rate that can be reached. The technology applied is significantly
higher powered than the scheme covered above and so there is not even an
attempt to explain it here, but those who really enjoy difficulty can
read the 45-page paper that introduced it! One of the authors
say that for this method to be cost competitive the numbers involved
would need to be rather large: ``Even if each digit was written on a
hydrogen atom, there would not be nearly enough room available in the
observable universe to write them down.'' That surely qualifies it
to count as galactic. 

\section{Regular expressions}
The separate chapter here on pattern matching also presents a problem
that might have seemed tame but is in fact galactic.

\section{Primality checking}
It is easy to check if a small number $N$ is prime -- just check
each potential factor up to $\sqrt N$. For larger numbers there are
fine and practially very sensible methods some of which rely on a supply
of (genuine) random numbers are deliver a result with arbitrarily small
probability of error, or that rely on currently unproven results in
number theory. But a scheme introduces by Agrawal, Kayal and Saxen\cite{AKS}
runs in time bounded by a polynominal in the number of digits it takes to
express the number and guarantees a correct response without needing to
make any qauestionalble assumptions. This is difficult in two senses!
The mathematics behind explaining and justifying it are somewhat tough,
and if it was implemented the suggestion is that its cost would would
be significantly slower than using probabilistic methods for all
numbers with no more than $10^{1000}$ digits. It is so labotious to
use that even for numbers of a few thousand bits (as used in many
cryptographic contexts) is is not really feasible at all as well as it
being hugely slower than other methods. So here we can report that this
exists, that it is a fine example of the levels of difficulty you get
when trying to get the very best solution to a problem, but that those
who want full details may first need to attend courses on number
theory!

\section{Minimum Spanning Trees}
A graph here is a collection of vertices with edges that join some of them.
A spanning sub-tree is a setset of the edges such that it is possible to
get from any vertex to any other one only traversing edges in that subgraph.
A graph may have weights (or costs) associated with each edge, and
then a minimum spanning tree is a spanning sub-tree such that the sum of the
weights on all its edges is as small as possible. Often in university courses
on datastructures and algorithms procedures by Prim and
Kruskal\cite{PrimKruskal} that find such minimum trees are presented. In
the more advanced versions of such courses improvements on the basic
versions of those using forms of priority queues known as Fibonacci
Heaps\cte{FibHeap} get discussed. If the graph hash $m$ edhes and $m$
vertices these schemes can guarantee to find a minimum spanning tree
within time proportioanl to $m + n \log n$ and that is typically good enough
for practical purposes. However a fancier method by Chazelle\cits{chazelle}
has a growth rate that scales as $m \alpha{m,n)$ where $\alpha$ is an
inverse Ackerman function and grows exceptionally slowly such that
for any conceivably feasible practical graph its value will be no
greater than 4. So until we are really at or beyond galactic scale the
costs of this method grow linearly with the number of edges regardless of
the number of vertices. As with the previous examples in this chapter this
represents a breakthrough such that for large enough graphs the method
will beat all the ones that can been developed before. As with other examples
in this chapter the details of the extreme method defy compact explanation,
but the overview is that they replace the Fibonacci heaps (which are themselved
somwhat messy!) with data structures known as ``soft heaps'' that mostly
provide priorty queue operations in a really cheap way but that
do not always yield the correct result. However they do offer bounds on
the frequency with which errors can be introduces, and the Chazelle method
as a while allows for that by making checks as it goes and where necessary
back-tracking to correct for mistakes. The presence of the inverse
Ackermnann function in the eventual cost bound gives clear warning that
this is all far from straightforward even though right here we are not
documenting just what that function is or how it arises!

\section{Perfect data compression}
When data of any sort is to be transmitted over a slow network or sored in
an archive it may well be good to produce a reduced size version of it such
that when the original data is eventually required it can be reconstructed.
The case considered here is where the original data must be recovered
perfectly -- that is in contrast to data compression often used with
images (\verb@.jpg@) and sound (\verb@.mp3@) where ending up with
a version that is in some sense ``close enough'' to the original will
suffice. There are many commonly used schemes for compressing data and
a numbers of then use the word \verb@zip@ in their names. But none of those
even pretend to achieve the best possible reduction in size.

There is a scheme that is in a sense optimal, but the big problem with it
is that it is even worse than the previous galactic algorithms just
considered, in that it can be shown that it is and will always be impossible
to write a program to implement it!

This scheme is known as Kolmogorov compression and the idea behind it is
really simple. A body of data is represented by the most concise program
that, when run, will regenerate it. Well that definition raises an obvious
issue: ``what notation should this program expressed in?''. From a
theoretician's perspective the response is that any notation at all
may be used. If then one wanted a version of the data to be decoded on
a different computer or one that provided implementations of different
languages all you do is construct code in the notation that will
suit this different computer that will emulate the one the data was
compressed by. This simulation code will be of some definite finite
size and so sticking it on the front of your compressed data will
only alter its simply-measured size by some additive constant.

The unsolvable problem that makes this scheme one only to dream of is
that there is no way to guarantee to find a shortest program
to generate some given output. This is a consequence of the fact that
given a program there is no algorithm that can guarantee even to tell
if it will terminate in finite time, far less give information
about its output. But this is only a practical limitation! One might
expect that something that was impossible to implement would not have many
uses, but amazingly that is not the case. Here are two that illustrate
the power of this concept:

\subsection{Testing for randomness}
A funadamental result is that if data is truly random then the most
concise why to express it is basically to put it in a simple
print statement. For instance this shows that a string of digits
such as those from 10000 to 1000000 within the decimal expansion of
$\pi$ are not really random, because this very sentence can be
viewed as a recipe from which they could be reconstructed. And
if desired the sentence could be preceeded by a program for
calculating digits of $\pi$. So the fact that the string of digits that
starts off $85667227966198867\ldots$ which may have no pattern
discernable to the casual eye could be spotted by Kolmogorov
compression as being extracted from the list of digits of $\pi$.

So suppose you have a sequence of values that are supposed to be truly
random -- such as numbers drawn each week in some lottery -- the
though here is that if somehow you could find a scheme that could
generate exctly that sequence where the description of your scheme
is notably shorter tham merely listing the numbers themselves then you
have good reason to believe that they were not random after all.
At least in some sense all the techniques used to try to assess
procedures that generate pseudo-random numbers are modelled on
this in that they look for patterns that represent traces left by
the mechanisms actually used to generate the numbers. But Kolmogorov
compression would uncover the implementation of the generating program
and in so doing show that the sequences were pseudo-random rather than]
genuine.

\subsection{How common are primes?}
The result sketched here uses the fact that almost all data must
be impossible to compress. This result emerges from counting how
many different possible byte-sequences of length $N$ exist and
concluding that any loss-free compression can only possibly map
that number of longer strings onto them. So all the other longer strings
can not be squashed. This line of reasoning does not indicate exactly
just which strigs will end up being mapped onto shorter ones and
which will not! But good practical compressions methods try to
arrange that the ones that are allocated short representations are ones
more liable to arise in practise.

Using this principle that most data is not compressible it is possible to
derive a result that shows that primes must be reasonably common. This
result is not going to be anything like as good as the ones that ``proper
number theory'' comes up with, but the fact that it uses data compression
as its main tool is perhaps witty!

So consider a really huge number that is written using $N$ digits. The
principle that most data can not be compressed says that in general
it will not be possible to find a way to describe it using significanly
fewer than $N$ digits. Well one alternate way of specifying the
number would be to list its prime factors, so for instance the
integer $856672$ would be presented as $2 \times 2 \times 2
\times 2 \times 2 \times 19 \times 1409$.
and now we imagine numbering the primes, so 2 is the first, 3 is the second
and so on, Then this representation of $856672$ might be written
as $(1,1,1,1,1,8,223)$ because 19 is the 8th prime and 1409 turns out
to be the 223rd.

Now if you imagine that primes are really very
uncommon then the index of a prime (eg 233) will be very much less than
the value of that prime (eg 1409) and so writing out the index will
use less space. That could potentially give a way of finding a way to
create a compressed representation of the number, and while that can
happen occasionally that has to be rare. This establishes that primes can
not be hugely rare.

Rather that attempting to get the sharpest possible result out of this,
pretent for a moment that the density of primes was such that the
$n$th prime had value comparable with $n^2$ which would mean that
its simply written value used about twice as many digits as were needed
for writing out its index. Then at least a simplistic reading is that the
compressed representation of a number a slist of the indices of its
factors will use half the number of digits plus a number of commas
based on how many factors are present. for big enough numbers this will
pretty well always be significantly shorter than the representation that
memly writes out the number simply, and this being impossible establishes
that primes must be more common that had been assumed.

This illustrated that even without being able to exhibit perfect compression
a consideration of it can be used as a basis for reasoning about
probability distributions and the like. But it remains the fact that
optimal compression is not merely galacticm it is uncomputable.