\chapter{Lambda calculus and Combinators}

Turing machines provide a model for computation that (perhaps unexpectedly)
can give really good insight into costs. But setting up programs for them
is not really anything like conventional programming and they do not
adapt well to day to day use. Minsky or Counter machines have their
behaviour expressed by flowcharts and come much closer to being amenable
to casual use, but they exhibit totally absurd run-times that render them
quite unsuitable for anything other than theoretical purposes. That leaves
a gap in the market to be filled by a formal model of computation that can
be reasonably easily adapted for use as a practical programming system. A
key candidate for this is called ``lambda calculus''.

The fundamental idea here ia that functions should be seen as the basic
building block for everything else. When first introduced this was accepted
as a neat theoretical idea and it led to what I will impolitely describe
as niche programming languages, but over recent years it has been recognised
as such a good idea that basically all modern programming languages
incorporate features that are directly derived from it. A {\em lambda
expression} denotes a function and is seen as freestanding entity. For
instance in earlier usage one could only introduce a function by a notation
that gave it a name, as in
\begin{verbatim}
    f(x) = 3*x + 1
\end{verbatim}
as a definition of a function called \verb@f@. With lambda calculus the
name f is not needed and we can write just this function as \\
\verb@    @$\lambda$\verb@ x . 3*x + 1@\\
where the name just after the $\lambda$ indicates what the formal argument is
and the section after the dot is the body of the function. About the only
action that can apply to a lambda expression is that it can be applied to
an argument, and the consequence of that is just about the same as when the
previous function called \verb@f@ is applies to an argument. It is almost
as simple as that!

Well there are two issues that need discussion that arise when several lambda
expressions arise together. The first rule is just to give a pedantic
confirmation that things happen the way you would naturally hope, and
it insists that when the body of a function has a value for the function's
formal parameter substituted in nothing improper happens. One version of
it starts by noting that in any lambda expression one could rename the
variable that is bound to anything else and so the example shown above is to
be viewed as entirely equivalent to \\
\verb@    @$\lambda$\verb@ y . 3*y + 1@\\
and the mapping from one text string to the other is known as alpha conversion.
Now if you applied out first lambda expression to an argument that was the
literal symbol \verb@x@ rather than a number there would be a potential
source of confusion about substituting for \verb@x@ in the body. All
possibiity of uncertainty can be overcome if one used alpha conversion on the
function just before applying it to make its formal parameter a new symbol
that could not conflict with anything. Another part of what is essentially the
same issue arises when the body of a lambda expression includes further nested
lambdas and variable names are reused. It is proper to apply scoping rules so
that inner lambda bindings take priority over outer ones, but even here use
of enthusiastic alpha conversion can keep all names separate and avoid
risk.

The second issue is that if one has a reasonably large nest of lambdas (and
some examples will arise later) it could be that there are several places in
the whole where a lambda could be applied to an argument. When there are
several options for the order to do these operations does it matter which
is selected and if so what is the proper strategy? On this there is some
divergence of opinion! From a theoretical point of view it is considered
proper that where their is a choice one should select the leftmost
outermost application. This is known as normal order reduction and if there is
any chance at all of continued application of lambda expressions to
arguments ever terminating with a form where there are not more available
than this guarantees to reach that state. The alternative is that
innermost lambdas are processed first. This can sometimes risk
unnecessary exploding or looping sequnces of operations, but is generally seen
as easier and cheaper to implement. So many but not all programming
lamngauges with lambda support follow the second path.

It is perfectly feasible to write out everything that is to be discussed in
this chapter using loads of $\lambda$ symbols, But for many people it
will be easier to work with a notation that feels more familiar (but that
can be expanded directly into the raw lambdas). A paper by P. J.
Landin\cite{next700} has as its title ``The Next 700 Programming Languages''
and introduced \verb@ISWIM@ (If You See What I Mean) as a sketch, and
so rather than latching onto any particular real or modern notation that
exposition used here will be based on that.
   
ISWIM starts by using a keyword \textbf{where} to give names to things.
To be a little pedantic here are a couple of examples and how
that would expand onto raw lambda expressions:
\begin{alltt}
 3*x+2 {\bf where} x = a + b       : (\(\lambda\) x . 3*x+2) (a + b)
 f(a+b) {\bf where} f(x) = 3*x + 2 : (\(\lambda\) f . f(a + b)) (\(\lambda\) x . 3*x+2)
\end{alltt}

The hope here is obviously that the ISWIM versions are un-threatening
and so will make things easy to understand. As well as \textbf{where}
that puts a definition after the use of something it will also be
possible to use \textbf{let} to put a definition first, and here we will
often use that with the intent that the definition provided should be
available to all subsequent input. What is more likely to cause some
concern to start with is that in this world functions are first class
citizens with all the rights and privileges entailed. In particular they
can be passed as arguments and returned as values. A major lesson that
lambda-calculus teaches is that this is liberating an powerful and
so it is used a very great deal.

The ISWIM examples above use function definitions and \textbf{where}
clauses to provide the structure of a program, and then have numbers
and arithmetic operations. As a prototype for 700 variant languages ISWIM
can imagine a wide variety of built-in data types and functions that
can apply to them, so that it can be tuned for particular application areas.
I will allow for the creation of user-specified data types. But it tends
to de-emphasise sequential programming, with its documentation noting that
in Algol 60 it was already possible to avoid that quite a lot by virtue
of allowing conditionals as expressions as well as statements. It can view
a sequence of several values as in \verb@A, B, C@ to be viewed as a single
item, and that item could be returned as the value of a conditional
expression or passed to provide three arguments for a function that needed
that many. The variant or variants of it used here will be ones tuned to be
good for describing things that can be done with lambda expressions!

From the tiny examples shown above it is possible to push explanations either
up or down in terms of the level of abstraction involved. Here we will first
show that lambda calculus using nothing more than function definitions and
even restricting those to the one-argument case is in fact capable of
modelling all computation. And that amazingly the steps it needs to take to
do this are all remarkably concise and may even not be terribly inefficient.
After that it will be explained how lambda expressions can act as a
convenient and well defined intermediate stage to be used by compilers while
mapping from source code down to some machine-specific level. And When that
has been done it is easy to observe that any programs in the languages that
those compilers support can be rendered as lambda expressions. A final
section will comment on a somewhat archaic but thoroughtly practical
language that was explicitly built to illustrate this but that has been
used to build large and complicated programs.

To build up an understanding of computation from lambda calculus we
start by noting that a lambda expression only has a single argument. In
much of what follows it is going to be convenient to think in terms of
functions with two or more arguments. These can be rendered using a
technique called ``currying'' after H. B. Curry\cite{curry} who popularised
it. We will write function definitions as if they used several arguments
as in
\begin{alltt}
   {\bf let} f a b c = a+b+c+2
\end{alltt}
using spaces between the multiple parameters rather than enclosing the
collection of them in parentheses.
We write in parentheses to show how we intend or expect to view this
as grouped. In this case both the description of the pattern for the
function (i.e.\ the bit to the left of the \verb@=@) and the additions in
the body associate to the left.
Then there is a bit of rather dubious renaming and a use of \textbf(where)
and tha name \verb@F@ that is really not quite proper but that turns the
main definition into one with a single argument which is then easy to
transform into a lambda.
\begin{alltt}
   {\bf let} ((f a) b) c = ((a+b)+c)+2
\(\Longrightarrow\)
   {\bf let} F c = a+b+c+2
   {\bf where} F = (f a) b
\(\Longrightarrow\)
   {\bf let} (f a) b = \(\lambda\) c . a+b+c+2
\(\Longrightarrow\)
   {\bf let} f = \(\lambda\) a . \(\lambda\) b . \(\lambda\) c . a+c+c+2
\end{alltt}
Even if the last line might look slightly awkward to start with it
shows that the ISWIM-style notation that defines a function that in a
certain sense has three arguments does correspond to respecable
lambda calculus.

Given that the first thing to be done is to show how easy it is to
provide truth values (i.e.\ {\em true} annd {\em false}) and a
conditional that tests them. Annd instantly from that {\bf and},
{\bf or} and {\bf not}:
\begin{alltt}
   let true a b = a
   let false a b = b
   let if p X Y = p X Y
   let and a b = a b false
   let or a b = a true b
   let not a = a false true
\end{alltt}
These can also look a bit strange to start with, but observe how very
concise they all are and then just verify them by writing down
combinations of them and using the funcction definitions as rules to
check how things behave. However there is one big caveat in all this --
it only works nicely provided you use normal order reduction, because
that manages to arrange that for instance when you write \verb@if p X Y@
that the evaluation of \verb@X@ and \verb@Y@ gets deferred until it can
be determined that only one actually needs to be processed. With
the evaluation strategy that deals with innermost expressions first
both \verb@X@ and \verb@Y@ would get expanded there and often that
would lead to a loop or other disaster.

Note that for the above to behave the value \verb@p@ tested by \verb@if@
must evaluate to either \verb@true@ or \verb@false@. In a bit there will
be a discussion about how that might be enforced.

Given truth values and conditionals the next feature of ``real programming''
to consider will be data structures.  A useful building block from which
pretty well anything else can be constructed will be the ``ordered pair'',
where we will have a function \verb@pair@ that creates on and then
\verb@left@ and \verb@right@ that retrieve one or other of the
two included items. Again the lambda forms that implement this are
amazingly concise and you just have to try following through what
happens when you use them to convince yourself that for instance
\verb@left (pair A B)@ will recover \verb@A@:
\begin{alltt}
   let pair A B f = f A B
   let left P = P true
   let right P = P false
\end{alltt}

There are two things that can be done instantly once you have pairs. The
first is to use clusters of them to build triples, 4-tuples and
larger structures with any fixed number of components. The second is
to pair an object with a tag (which might be a truth value) so that
as a while it can have two (or more) possible shapes. As an
illustration of this consider a way of handling lists that can have
and number of elements (including zero):
\begin{alltt}
   let emptylist = pair false <anything>
   let node A B = pair true (pair A B)
   let isempty L = not left P
   let first L = if (isempty L) <error case> (left (right L))
   let rest L = if (isempty L) <error case> (right (right L))
\end{alltt}
which exposes the fact that dynamic structures like this have
to carry extra information so that it is possible to detect when
the end of a list is reached. A similar scheme could describe trees.
The sections in the above that are written in angle brackets are
parts that ought never to be used so anybody confident that their
code would not contain any errors could use absolutely anything
to fill those gaps.

Next comes a representation of numbers and arithmetic. Well one
scheme that could handle positive integers tolerably well would be to
express them all in binary and use a list of the bits, with \verb@true@
used for \verb@1@ and \verb@false@ for \verb@0@. With that representation
arithmetic operations would have costs driven by the bit-length of
the numbers, and really that is a proper cost even though we tand to
be used to thinking of arithmetric as having unit cost per operation. This
modelling comes much closer to revealing how many electronic components
will be activated in performing an operation.

But coding up binary arithmetic is a bit tedious, so it is fun to show that
it is seriously easy to make pure lambda calculus explain the behaviour
of integer arithmetic\footnote{To be more pedantic natural number
arithmetic, i.e.\ only using positive values} really neatly.
Rather than thinking in terms of zero, one, two, three and so on it
is good here to thing in terms of never, once, twice, thrice and the like,
and a ``number'' can then be a function that applies a second function
multiple times:
\begin{alltt}
   let never f a = a
   let once f a = f a
   let twice f a = f (f a)
   let thrice f a = f (f (f x))
\end{alltt}
and this pettern allws you to have a function corresponding to any
number of copies of \verb@f@ you could want to set up. With this
representation of numbers a scheme to do basic arithmetic is almost
frightendingly simple!
\begin{alltt}
   let increment n f a = n f (f a)
   let add m n f a = m f (n f a)
   let multiply m n f a = m (n f) a
   let power m n f a = n m f a
\end{alltt}
and also a test for zero is remarkably easy to arrange
\begin{alltt}
   let iszero n = n f true where f x = false
\end{alltt}

Again one can appreciate how compact all the above are even without (at first)
understanding how they work. And to get insight into the working it is
possible to start with simple expressions such as \verb@power twice thrice f x@
and follow through the definitions to observe it expanding until it ends up
as (in this case) \verb@f(f(f(f(f(f(f(f(a))))))))@ which used \verb@f@
$8 = 2^{2}$ times. Or \verb@iszero twice = twice f true = f(f true) = false@

There is one thing to be done here that is slightly more tricky, and one
issue that has so far been swept under the carpet. The tricky issue is
a function \verb@decrement@ to find the predecessor of a n. It should be
expected that this will not be quite so straightforward because \verb@never@
does not have a proper predecessor. There are however several ways of
managing this. The one shown here is not the standard one but is perhaps the
most compact -- but it relies on the precise way in which \verb@increment@
has been coded and it achieves its result by handing that an argument that
is rather improperly not a number but that has been chosen so that the
resuly returned is \verb@never@. See how that can be done, starting with
a letter X standing for what we will invent:
\begin{alltt}
  increment X f a = never
  increment X f a = X f (f a) = never
  let X a b = never
\end{alltt}
and the three lines above that intoduce a function \verb@X@ that
in just this one contexe serves as $-1$ we end up with an implementation
\begin{alltt}
  let decrement n f a = n f X a where X a b = never 
\end{alltt}
Concise it possibly confusing\footnote{
A scheme that does not cheat by passing things to the increment function that
are not numbers works by having a function that accepts an ordered pair as
its argument: \texttt{let inc (a,b) = (a+1,a)}. If this is applied $n$ times
to a starting value (0,0) the result is ($n$,$n-1$) so the second component is
the predecessor of $n$}

Any reasonable programmer will see that given decrement it will now be
pretty trivial to code up subtraction and then division. If negative numbers
are needed that can be handled using a sign-and magnitude representation
by using \verb@pair@ to glue an explicit sign only every number.

Now for the issue that had been avoided that far (but when you look back
at all the fragments of code to date it does not impact them). In raw
lambda calculus the functions do not have names. That means there is no
obvious way for the definition of a function to refer back to the function
itself. In ISWIM you could have felt tempted to write things rather like
\begin{alltt}
  let factorial n = if n=1 then 1 else n*factorial(n-1)
\end{alltt}
as a simple recursive definition. On expanding that to lambda
calculus the use of the word \verb@factorial@ within the body
of the function will be improper and it certainly has no simple
way to relate to the name you happen to be using for the function.
So it looks as if recursion is going to be a problem! Happily this
can be resolved. Often the way of sorting it is thought of as a bit
strange. The presentation here is based on thinking of recursive
definitions as just a notation for programs of infinite length
that unwind it:
\begin{alltt}
  let f1 n = if n=1 then 1 else n*f2(n-1)
    where f2 n = if n=1 then 1 else n*f3(n-1)
      where f3 n = if n=1 then 1 else n*f4(n-1)
        where f4 n = if n=1 then 1 else n*f5(n-1)
          ...
\end{alltt}
and provided that chain of nested definitions is continued to
a depth that is at least as great as the value of an argument $n$ we are
about to use this will do just what is needed. This sort of expansion
can be applied to any recursively defined function\footnote{
ISWIM insists on the use of \texttt{let rec} when a recursive function is
being defined to highlight that something special is going on.}.

The magic that makes it possible to create unboundedly nested things in
raw lambda calculus is a function traditionally called \verb@Y@. Its
behaviour can be characterised with the identity
\begin{alltt}
   Y f = f (Y f) = f(f(f(...
\end{alltt}
but because this is an explicitly recursive presentation it can not be
used directly. However there are many ways in which ISWIM or lambda
calculus can provide a \verb@Y@ function without needing recursion,
and the simplest is
\begin{alltt}
  let Y f = g g where g h = f (h h)
\end{alltt}
In ISWIM notation this is really concise and rather easy to understand!
Just simply following the steps we have \verb@Y f = g g = f (g g) = f (Y f)@.
ands the ISWIM definition did not mention \verb@Y@ within the right hand
size of its definition so it translated into 100\% valid lambda calculus.

The general use of \verb@Y@ is that recursive function definitions can be
expanded using the following rule:
\begin{alltt}
  let f x = ... f x' ...
\(\Longrightarrow\)
  let f = Y G
    where G f =
      F where F x = ... f x' ...
\end{alltt}
and the key thing here is that the recursive reference to \verb@f@ now
has a meaning because it is talking about the argument to G, and if you
trace through expanding the definitions out a few times you can see that
this ends up being exactly what \verb@f@ was to end up as.

We now have boolean logic, data structures, arithmetic and recursion and
with those it is pretty easy to code up anything else that can be
imagined\footnote{Well interactive input and output, file-system access and
the like are not parts of the world of concern just here!}. And the
mapppings from them down onto the extraordinarily simple basic rules
of lambda calculus are concise and in general not too unpleasant to work with,
so if anybody {\em really} wants to reason about the behaviour of code
starting from first principles it can be reasonable to select lambda
expressions and their behaviour as those fundamental principles. But
now numbers and all the rest can (when necessary) be explained in this
rather gory manner it is perhaps reasonable to consider a version of
ISWIM that has all such features as built in primitives that can be
implemented using traditional computer hardware and reasoned about at their
own leval of abstraction. The fundamentalist view of lambda calculus then
just sits in the background providing assurance that the foundations for
all that is built are really secure.
