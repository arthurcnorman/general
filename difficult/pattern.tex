\chapter{Simple Pattern Matching}
\begin{quotation}\textit{
Almost every user interface provides some sort of ``search'' facility,
and a powerful search needs to cover more than merely looking
for fixed strings. It needs to seek text that matches some sort of
user-specified pattern. A mimimum would be to allow wildcards within
the target to be sought. But across a wide range of computer applications
there are variations on a form of pattern known as a ``regular expression''.
Using those can feel easy and natural. But as is the case with some much
in the computational world, these have more depth that is at first obvious.
}\end{quotation}
Sometimes you might want to search within some text but what you want to
find is not just a fixed string. Perhaps it can allow options or
repetition of sub-parts. Perhaps you want to put some sort of wild-cards
into the pattern that is your target. There is a very well established
scheme for setting up patterns for use in cases like this, and variations on
it. Very many programming languages and even dialog-boxes in user interfaces
use at least subsets of it. So for instance the pattern \verb@*.jpeg@ may
be used to let you look for all files with the ``\verb@jpeg@'' suffix,
while at least in a Linux shell the pattern ``\verb@*.\{cpp,h\}@ will match
names that end in either \verb@.cpp@ or \verb@.h@. A fuller scheme used for
pattern matching as part of the language PERL and available through libraries
in almost all other programming languages as a bit more formal. A
pattern is built up starting with the very simplest: patterns that
consist of and match just one letter\footnote{It can also in fact be
useful to have a basic pattern that matches an empty string.}. These
simple patterns are combined using three constructions, If P and Q are
existing patterns then one can write
\begin{itemize}
\item $P Q$ -- this is a pattern that matches anything that can start with
a sequence that matches P and follows that with one that matches Q. Obviously
the very easiest use of this is that it means that you can write a sequence
of individual letters and they form a word to be spotted;
\item $P | Q$ -- here we accept anything that matches either $P$ or $Q$.
So for instance \verb@cat|dog|rabbit@ matches strings that name creatures
suitable as pets, and \verb@p(e|a)t@ illustrates that it is sometimes
useful to have parentheses to group things.
This pattern will match either \verb@pet@ or how you might treat one,
i.e.\ \verb@pat@. If you need one of the characters \verb@(@, \verb@)@
or \verb@|@ in the alphabet you are matching over some way of distinguishing
raw characters from punctuation used to build up the pattern will be
called for.
\item $P*$ -- This is the big one, It indicates an arbitrary repetition
of the pattern $P$. So it is in effect equivalent to $(| P | P P | P P P |
\ldots)$. Note there the initial option of no instances of $P$, i.e.\ of
this matching the empty string. A really simple instance of this would
be \verb@B(an)*a@ which matches \verb@Ba, Bana, Banana, Bananana@ and so on.
\end{itemize}

There are two viewpoints that can be taken about this. One is a practical
one that adds a number of shorthand notations for things one might
frequently want to do. A particular instance of this arises because these
patterns (which are referred to as ``regular expressions'') provide
an excellent way of characterising the ways in which tokens or symbols can
be written in programming languages, and there are software tools that take
a list of patterns and create a program that splits textual input up based
on the. A first exxtension to notation that is used there is being able
to give a name to a pattern fragment and then use it later. In the programs
\verb@lex@ and \verb@flex@ one can name a fragment and then to refer to
it you put the name in braces. You also enclose literal text in your pattern
in double quotes. So for instance:
\begin{verbatim}
    digit    "0"|"1"|"2"|"3"|"4"|"5"|"6"|"7"|"8"|"8"
    number   {digit}{digit}*
\end{verbatim}
gives a pattern for any (non-empty) string of digits and calls it ``number''.
This example motivates two further expensions which clerly do not alter
the range of patterns that can be expressed but that can make the
presentation of the regular expressions concerned much more compact.
Enclosing a collection of characters in square brackets and allowing
character ranges is a help. If the opening square bracket is followed by
\^{} then the expression is treated as if was a square bracket form
enclosing all letters in your character set except the ones actually shown.
WIth this the tabluation of digits becomes just \verb@[0-9]@. The second
expension allows for the fact that \verb@*@ can indicate zero or more
uses of the pattern that preceeds it and sometimes as here you want at least
one. Replacing the \verb@*@ with \verb@+@ does that. Hence you can now write
\begin{verbatim}
    digit    [0-9]
    number   {digit}+
\end{verbatim}
Note that this could be textually expanded to the slightly clunky but
basic for for regular expressions, so the extended notations are in general
a matter of convenience rather than things that bring genuine new
capabilities. And in that spirit here are a few more useful extensions that
similarly do not alter the range of patterns that can be expressed but that
may make it easier to specify them.
\begin{verbatim}
    P & Q
    ~ P
    .
\end{verbatim}
The idea is that the first of these will match every pattern provided that
both \verb@P@ and \verb@Q@ do, while the second matches any input that
\verb@P@ would fail to accept. The single dot (\verb@.@) will match any
single character, and so of course \verb@.*@ matches either anything
at all (including nothing).

While it is fairly straightforward to show that adding these constructs does
not add any ability to match new sorts of pattern -- all they do is
make it easier and more flexible to specify them -- the notes here are
not going to explain the details there. Head for a suitable textbook if you
need to know exactly how it can be done! But the typical places on
your computer that provides support for regular expressions will typically not
support these last two because in fact they unlock levels of practical
difficulty that are hard to comprehend.

Thus far the use of regular expressions to provide patterns that you can
try to match against input text seems really rather easy despite the
pessimistic statement above. So here to give some insight into just
why adding those two last capabilities is so bad let's state what
problem it turns from tolerable into being solvable in principle and
theory but utterly dreadful in practise.

The sections here will next work through showing that it is possible to
set up a regular expression of reasonable size that such that the
only strings it can match will be absurdly long. Once that is done
it becomes possible to argue that while answering various questions
about them and their relatives is theoretically posisble, the cost of
doing so will be at least as great as the length of the shortest string
they match, and hence is beyond all feasiblity now and for ever.

The way of doing this will be by using the term ``ruler'' to characterise
an expression that only matches strings of some given lenght $L$, and
showing that if you are given a rules for $L$ you can derive one for
a length of the order of $2^{L}$ such that the new expression is only
some constant factor lerger than the original one. By iterating this
process you can obtain rulers of length $2^{2^{\ldots L}}$ for any height
tower of powers of 2 that you want, and that leads to truly huge values
rather rapidly.

To illustrate the process we will start with a ruler of length 3, and
the simple regular expression \verb@x x x@ consisting of just 3 characters
does the job. The general concept that will be set up is based on
counting in binary with (in this case) 3 bit numbers, which may be
presented to start with on two lines as
\begin{verbatim}
      #000#001#010#011#100#101#110#111#
   ...#xxx#xxx#xxx#xxx#xxx#xxx#xxx#xxx#...
\end{verbatim}
Here the upper line can be seen to be counting and it uses the hashes to
keep the separate binary values neatly apart. The lower line uses our
ruler and will help to enforce the regular patter of where the hashes
appear. It can be rendered as the simple expression
\verb@#(xxx#)*@ which just takes our ruler and sets up repeats of
it separated by \verb@#@ characters . Well this looks like two strings,
one for counting and one for ruler. So then we can use the language of
regular expressions we interleave the two so we have a single string where
odd characters are from the top line and even from the bottom:
{\footnotesize
\begin{verbatim}
   ##0x0x0x##0x0x1x##0x1x0x##0x1x1x##1x0x0x##1x0x1x##1x1x0x##1x1x1x##  
\end{verbatim}}
This may be less easy for a human reader to decode and so in the
following presentation we will sometimes use the 2-line format, but
what it will always mean will be the flattene out version. Note
that with our ruler of length 3 the string we have here is
of length $2 \times (3+1)\times 2^{3} + 2$ where the $3+1$ and the final
$_2$ come from counting the hashes and the extra factor of 2 is because
there were two lines or text merged. Calling this just $2^3$ is something
of an understatement but that does not alter the main thrust of the
presentation, which is that this will allow us to build extraordinarily
long rulers.

Initially it might seem that setting up regular expressions that constrain
text strings to count in binary is going to be hard. The way of doing so is
is to consider what strings are {\em not} counting sequences. So we will
establish a set of regular expressions exact of which characterises somenthing
that we do not want to see, and using negation and the ``and'' operator
ensure that none of them apply. What remains will only be able to be
our counting sequence. This of course is the just what Sherlock Holmes
taught us: ``When you have eliminated all which is impossible, then whatever
remains, however improbable, must be the truth.''.

To start with it makes sense to enforce the block format of the
string, which amounts to demanding that hashes in the top line can only
be in places where they lie above hashes in the bottom line. Well any string
that violated that rule will have either a hash above a non-hash
on a non-hash above a hash somewhere. If just deal with the first of those
it can be desribed using a pattern that carefully uses pairs of
characters everywhere
\begin{verbatim}
   (..)*  (# (~#)) (..)*
\end{verbatim}
This little expression and its friend that matches the bottom to top
will form part of the expression we are building to describe our long
string. As written above it is only 17 tokens long. If you needed to
use more primitive regular expressions each wildcard charactter \verb@.@
might expand into something like \verb@a|b|c|d...0|1|#@ and have a size
proportional to the number of characters in your alphabet, but that again
us just a constant.

The next consaint can be expressed positively and is that the
string we are going to match must start at \verb@000@ anmd stop
at \verb@111@. Again this is easy:
\begin{verbatim}
   #. (0.)* #. (..)* #. (1.)* #.
\end{verbatim}
where this version demands that our initial ruler is of length at least 2
so there are at least two hashes between the first and last segmnents, This
wors because \verb@# 0* #@ insiats that there are only zeros in that region,
and similarly for \verb@1@s at the end. And the previous rule has made cartain
that the \verb@#@ marks all line up and that forces those strings of zeros and
ones to be the desired length. Note that all the messy things like
\verb@(0.)*@ and indeed all the dots here are just arranging that we only
paty attention to top-line (eg odd position) chartacters. because that is
really just a bit of technicality in future we will present things in
the form as if they were only applied to the top line and the extra mess
to ignore the interleaved bottom line will be assumed to be applied:
\begin{verbatim}
   # 0* # .* # 1* #
\end{verbatim}

Looking ahead it will make sense te enesre that we only count through the
binary sequnce once, so that it is not possible to have a second section
consisting all of zeros. This is also rather easy if we make it a pattern that
we will say must not apply that puts two separating hash marks ahead of
a block of all zeros. It is written here ignoring the issue of the lower
ruler line... 
\begin{verbatim}
   .* # .* # 0* # .*
\end{verbatim}
which is anything then \verb@#@, more anything and then a block like
\verb@#000#@. By disallowing that we will count just once.

That leaves what may feel like the hard part which is to ensure that
sequential blocks of digits count. It feels easiest to first explain
what must happen, but the fragments of pattern we end up using will all
need to be set up to disallow anything else. To increment a binary
number one looks for the rightmost \verb@0@ and of course that means
any digits to the right of if must alll be \verb@1@s. The next number
up must preserve all bits to the left of that key zero, flip the zero into
a one, and turn all the trailing ones into zeros. A way of handling this
is to imagine that corresponding bits in the number and its successor
(which must be a distance apart set by our ruler) are underline and
displayed in bold, then four cases apply which are shown a bit
informally here: \\
\verb@       ...@\textbf{\underline{0}}\verb@ ... 0 ... # ... @\textbf{\underline{0}}\verb@ ...@\\
\verb@       ...@\textbf{\underline{1}}\verb@ ... 0 ... # ... @\textbf{\underline{1}}\verb@ ...@\\
\verb@       ...@\textbf{\underline{0}}\verb@     1*    # ... @\textbf{\underline{1}}\verb@ ...@\\
\verb@       ...@\textbf{\underline{1}}\verb@     1*    # ... @\textbf{\underline{0}}\verb@ ...@\\
The first two cases have at least one zero after the particular
character and insist that the character remains unchanged, while the
second two are when there are only \verb@1@s to the right in which case the
bit is flipped. These cases are mutually exclusive but between them cover
all possibilities.

The key trick is clearly to get the effect of the underlining set up
in our petterns, and that can be explained in a ``two row'' presentation
again. where for that first the above four cases what must be avoided is
\begin{verbatim}
   .* 0 .* 0 .* # .* 1 .*
   .* #     xxx.     # .*
\end{verbatim}
which can be read out as the lower line using the ruler \verb@xxx.@ to insist
that the two \verb@#@ marks are the length of our ruler plus one (to allow for
the \verb@#@ in the main pattern) apart. The top line looks for zero which is
followed by at last one zero before a \verb@#@, and that after the \verb@#@
there is a one that has been forced to be just the right distance away.
Matching a \verb@1@ there captures the case that is wrong, so saying that this
pattern does not match leaves only the good cases. The ruler line used here
is not the same as the one used before in that it puts in only two markers
that are properly separated, and the leading and trailing \verb@.*@ parts
say that it can match and indicate that separation anywhere. Ah well --
adding this in will mean interleaviung these two patters again, and the
interleaving with the previous repeating ruler will have to remain in place.
The result is that characters are going to end up being worked on in
blocks of 3 or perhaps 4 rather than individually. This just makes
the big ruler that we are creating with our counting scheme a bit longer
yet and is not a big cause for worry.

Each of the fragments of regular expression introduced here are rather
small even when all the interleaving stuff is allowed for, and in the end
we have shown how to create a pattern whose size is some modest multiple
of the size of the original ruler, but that will only match strings that
are exponentially larger.

So why might this be a worthwhile exercise? Apart of course
from it being a demonstration that pattern matching can do more that
was first apparent. Well it can be part of the process of showing that
understanding particular patterns can be hard. Begin with a fundamental
result about these patterns, which is that any regular expression will
have a corresponding deterministic finite automaton that can recognise
exactly the strings that the pattern will match. Well to rephrase that
in less tecnical language for any pettern there is a really simple program
that can be used to apply it to input strings. The program is
basically of the form:
\begin{verbatim}
   int state = 0;
   for each character in the input
     state = transitionTable[state, character];
   return isAcceping[state];
\end{verbatim}
where the transition table is used to adjust the state as each fresh
character is presented and the \verb@isAccepting@ vector reports
at the end whether the string matched. It is clear that the work done
for each input character is rather small -- just accessing the
array, and so this is liable to be really efficient way of performing
pattern matching. There are readily available programs\footnote{notably
lex and flex} that can build the tables and they will tend to be a bit
cleverer than using a simple 2 dimensional array for the transitions would
be because many entries in the full rectangular block would never be used.

Both for practical and theoretical purposes it is wothwhile to consider
how many distinct states will be required to build a program of the above
form that will correspond to a given pattern. One bound that is pretty
clear cut is that there must be at last as many states and the shortest
string that matches the patterm. To see that imagine the sequence of
states encountered in the process of a succesful match of such a shortest
string. If tere were to be fewer atates than the length of the string them
one of the along the way would have to be repeated, as in
\begin{verbatim}
   ABCDEFDGHIJ
\end{verbatim}
where state \verb@D@ has been entered twice. Well if that happened it
would be possible to provide input that ran up to that situation and then
continued ona  home run, omitting all the steps between the first and
second encouter with the repeating state.
\begin{verbatim}
   ABCDGHIJ
\end{verbatim}
This is a shorter string that has just been accepted, contradicting the
assertaion that we had started by looking at the shortest matching
input.  What we can read off from this observation is that provided we
allow extended regular expressons with a negation operator there are
tolerablly concise patterns that would necessarily lead to super-galactically
large numbers of states. This is a pretty convincing reason not to allow
negation in practical applications of these patterns!

So next consider a challenge that if it was readily solvable would be
of great use: given some pattern expressed as a regular expression
is there another more concise pattern that matches exactly the
same set of inputs? From the perspective of a theoretician this can
be addressed by emulerating all possible regular expressions with
all the ones using $k$ characters before the ones with $k+1$ characters,
amd with ones that are the same length in alphaberic order. Over any fixed
alphabet of the characters you are working with this is clear-cut.
Now for each such you want to test if it happens to match the same
inputs that your original one did. Well if I have two regular expressions
$P$ and $Q$ then the language $(P\&~Q | Q\&~P)$ will match any things that $P$
does but $Q$ does not and vice versa. So if the two expressions do the
same thing this new one I have just set out will not match anything at all.
But that is something that there is a clear-cut way of checking for.
Make the transaition table for this new composite expression and supposing
it turns out that there are $N$ states then if there is any string that
it accepts at all there must be one of length at most $N$. This is just
the flip-side of the previous remark about lengths of matched strings
and the number of states. If your alphabet if of size $k$ then there
are only(!) $k^{N}$ strings of length $N$ to check and if none of those
match then nothing will. The explanation here is a bit needlessly inefficient
but it demonstrates that in a finite amount of work one can tell of
two patterns match the same set of strings and that then by exhaustive search
in a finite amount of time it would be possible to exhibit the most
concise pattern for any particular behaviour. But although tis shows that the
challengs is decidable in the sense that one can have a computer program that
would guarantee to address it and eventually complete its work with a
correct result, the resources needed to achieve that might well be
truly ridiculous.

The explanations given here show how huge rulers and hence regular expressions
that necessarily load to transition tables with huge numbers of states csn
arise. They do not complete the proof that there can not be some clever way
of reasoning about e.g.\ equivalence between patters that manages to sidestep
generating the associated transitionm table. And details of that will be
left as a futher literature search or research project for the reader, But
a key step that it may by now be easy to recognise is that the sorts of
techniques explained here that block a string into segments each representing
the next binary number on from the previous can be modified so that the
string is split into blocks such that each represents the next state
of a fairly arbitrary computer one step on from the previous. It would
be normal to set this up describing a Turing machine with a tape whose
length is limited to the size of our ruler. Given that and the large
amount of theory known about the problem of telling of a Turing machine
will halt you can imagine that formalising a statement that there can be
no amazing short cuts and that something very much like building the full
state transition table for the description is going to be necessary to
tell if certain messy patterns will match anything at all.

In writing this the authors here want to leave the reader understanding that
there is always more to be looked into!
