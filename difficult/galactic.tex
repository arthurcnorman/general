\chapter{Galactic algorithms}

Gaalactic algorithms are ones that have costs that grow slowly with the
size of their input to an extent that for large enough cases thay will
do better than simpler methods, but that suffer from such large
overheads that they will never become practically useful for any
plausible cases. The finest ones will provably achieve the slowest
possible growth rate for the problem that they solve. Some of the
most impressive have overheads that can be expressed as a constant factor
multiplier in their cost formula with that constant exceeding astronomical
proportions. There will obviously be case that come close to fitting into
this category. Because such methods are not of a lot of practical use one
cound be tempted to view them as frivolous, but where they represent
a better growth rate than any other known method or are provable optimal
in that respect they can both be of intellectual interest and can provide
ideas to guide work towards schemes that can be useful in the real world.

A number of galactic algorithms illustrate how much more complicated
things get if you aime for the absolutely best possible method, but
here we start off explaining one that has been known for some time
and is neither terribly complicated nor terribly expensive, but that
despite it giving a theoretical speedup will onlt achieve that in cases
pushing against the size limits of the biggest available computers, and
even then its advantage will be slight.

\section{Matrix Multiplication}
The straightforward scheme for multiplying a pair of square matrices of
size $n \times n$ has cost that grows proportional to $n^{3}$. Our first
galactic algorithm represented a breakthrough discovery that this
growth rate was not the best possible, and that a tolerably straightforward
scheme could achive a cost that grows as $n^{\log_{2} 7}$ which is
about $n^{2.80735}$. To show how this is done it is sufficient to
consider the very simple case of forming the prodict of a pair of $2 \times 2$
matrices $A$ and $B$. The four elements of their product as evaluated
in the classical manner aee just $A_{11} B_{11} + A_{12} B_{21}$ and three
rather similar looking values and a total of 8 multiplications and 4
additions are used. A paper by Strassen\cite{strassen} instead managed to
show that using just 7 multiplcations that worked on carefully chosen sums
and differences (e.g.$($A_{11} + A_{22})(B_{11} + B_{22})$) it was possible
to express all the values needed in the product matrices as sums and
differences involving these 7 values. Here we leave finding exactly how
to do that as either a puzzle to work on or a topic to research! A variety
of different schemes have been found, all ending up with just 7
multiplications but some with more and some with fewer additions.

Once the base case of $2 \times 2$ matrices has been handled large case can
be treated by partitioning each matrix into four blocks (padding out with
zeros if necessary to make those blocks all square and all the same size)
and coping there by using just 7 multiplications (which are now matrix ones)
on those half size blocks. An analysis of the growth rate in time spent
using this scheme fairly clearly suggests that doubling the side of
the matrices multiplies the amount of work by a factor of 7, hance the
$n^{\log_{2} 7$ result. This is clever and quite elegant, and not even
too hard to program. But as against the classical method it struggles
even to break even until matrices are larger than almost all applications,
amd it certainly does not deliver truly valuable speedup for some way
beyond that. A further concern with this approach is its impact on how
rounding errors propagate through the calculation.

But this is not the end. Since the Strassen work others have sought
to find schemes that are at lease assymptotically faster, with a long
sequence of reports og gradually reduction of the exponent in the
cost function. As of 2026 the best growth rate that has been reported is
around $n^{2.37134}$ which looks like a distinct improvement over
Strassen. So far nobody has been able to show what the ultimate limit
for solving this problem will be. Most of these improved schemes work
by splitting large matrices into more then 4 blocks and all of them
are even further from practicality then the Strassen method.

\section{Integer multiplication}
Multiplying integers is a pretty fundamental operation, but here to
make things harder the emphasis will be on large integers -- typically
ones with from hundreds to millions of decimal digits. Things start
off in a style almost parallel to the matrix multiplication one,
with first the simple ``schoolbook'' methos that will form the
product of a pair of $N$ digit numbers using around $N^2$ operations.
The first improvement on this is dus to Karatsuba\cits{karatsuba} and
works by observing that for 2-digit numbers one can form the
partial products $a_{1}b_{1}, a_{1}b_{2} + a_{2}b_{1} and a_{2}b_{2)
using 3 rather than the obvious 4 multiplications by calculating
$a_{1}b_{1}, (a_{1}+a_{2})(b_{1}+b_{2}) and a_{2}b_{2} and doing a couple
of subtractions. By taking large numbers are splitting each in half
this can be applied recursively leading to a method where costs
grow like $N^{\log_{3} 2$. This scheme can become proper to
use when numbers exceed say 10^{300}, but the exact break even point will
be quite sensitive to details of the computer used and just how everything
was coded. So this not a galactic algorithm.

The next scheme of note may start to be competitive when multiplying
numbers of magnitude around 10^{5000}. The exact point where it becomes
realistic can vary quite a lot and 5000 digits is perhaps better
characterised as stellar rather than galactic, so this is a method that
is at least relevent in some slightly specialist cases. It is however
much harder to explain in elementary terms, and so what will be presented
here is an overview perhaps uses enough technical language to show
that it starts to count as ``difficult'' or at the very least ``reasonably
advanced''. So for those who do not recognise the words used here this
will just show how much messy technology a simple-seeming task such as
integer multiplication can involve. For those who then want to follow
through by re4ading textbooks or scouring the web it may give a broad
overview so they know how the things they read up on fot together. And
those who already understand this well can just smugly skip to the next
section.

Sch\"{o}nhage-Strassen works by first padding its integer inputs so that
each have a suitable power of two bits. At least as an abstract algorithm
if expresses its operations in terms of bits not digits so it is kept
fully honest and does not hide costs within arithmetic on digit-sized
units. Then if the numbers are of length $N$ it clumps the bits into
clumps of size around $\sqrt N$ and views each clump as a digit, and so
there are around $\sqrt N$ digits. Well there can be some pedantry about the
exact size of digits that is needed but the decomposition as described
here is close enough for use in this overview.

The next issue is that it views each collection of digits as a vector
and it takes a Fourier Transform of it. So what is a Fourier Transform?
Well it amounts to multiplying a vector of length $K$ by a special form
of matrix where the entries in the matrix are all powers of a value $\omega$
where $\omega^{K} = 1$ but no lower power of $\omega$ has that value. In
the normal world one would say that $\omega$ is a complex $K$th root of unity,
and that it is primitive, so at first it looks as if complex numbers are
involved. However to avoid them (and any risk of creeping rounding error
that could hurt if floating point arithmetic was used), Sch\"{o}nhage and
Strassen used modular arithmeic. In other words they have some number $P$
so that after any operation on integers they just retain the remainder
when the natural result is divided by $P$. A big part of their cleverness
is that by choosing $P = 2^{M}+1$ for a suitable $M$ two wonderful things
can be achieved. First it becomes possible to use 2 as the value for $\omega$
and that means that multiplication by powers of $\omega$ can be mechanised
as simple cheap shift operations. Secondly the remaindering operation
where a double length integer is to be reduced modulo $P$ can be performed
by just subtracting the top half of the number from its bottom half.

One interpretation of what a Fourier Transform does is that it views
its input vector as the corfficients in a polynomial and evaluates that
polynomial at all the powers of $\omega$. If that is done to two polynomials
and the resulting values are pairwise multiplied the result will be
the values of the product polynomial at all those points. An inverse
Fourier Transformation will act as interpolation and recovers the
coefficients of the product polynomial. This can be re-interpreted
as an integer following some simple carry operations. A further key
feature of Fourier Transforms is that on using vectors of size $K$ can
be completed using around $K \log K$ arithmetic operations. Of course
here each of those arithmetic operations is using modular arithmetic
modulo $P$ where $P$ is a pretty huge multi-digit number -- but because
of all the powers of 3 involved that is not in fact seriously painful. In
fact the worst part of all of this is the pairwise multiplication of elements
of the transformed vectors and that generally needs to recurse into a
further layer of use of the whole procedure -- at least until things get
small enough that classical methods will win.

The ``polynomial product'' produced by the Fourier Transform scheme has
been computed modulo $P$ but we want exact and correct integer results.
Well provided $P$ was greater than any value that could legitimately
arise in that product all is well and the calculated values will be
the perfect integer results as needed.

The above explanation know it is a sketch. It has omitted the need to
pad various things up to powers of 2 and the proper care about splitting the
inputs up so that the value of $P$ used is big enough to ensure that
results are correct. Those details of course matter in a proper formal
explanation of the algorithm and in any implementation of it, but perhaps
they repesent details best defered to a second reading about the method.

The Sch\"{o}nhage-Strassen scheme for multiplying $N$ bit numbers has a
cost that grows proportional to $N \log N \log \log N$. Because it is
concerned with truly gigantic input numbers (i.e.\ values of $N$) it
becomes proper to worry that if digits making of numbers are stored
in computer arrays of length $N$ then index arithmetic used to work out
which digit to touch next can become beyond th4e scope of single
computer operations. For instance a modern 64-bit computer will not
cope trivially with arrays with more than $2^{64}$ elements! So the proper
analysis of costs here needs to be done in terms of some suitable abstract
machine that does not have any limita at all on the bulk of data it
can store and maniplulate. Variations on Turing machines are commonly
used, and amazingly some of them can deliver the cost growth rate
noted above.

This scheme, however messy, is still used in some practical applications.
But it is not the end of the road. A succession of authors have described
yet more elaborate schemes that reduce the $\log \log N$ term in the
cost function, culiminating one\citeHarveyvanderHoeven} where the cost
is precisely proportional to $B \log N$. This is a great theoretical result
in that it is conjectured (but not yet proved) that this is the best
growth rate that can be reached. The technology applied is significantly
higher powered than the scheme covered above and so there is not even an
attempt to explain it here, but those who really enjoy difficulty can
read the 45-page paper that introduced it! One of the authors
say that for this method to be cost competitive the numbers involved
would need to be rather large: ``Even if each digit was written on a
hydrogen atom, there would not be nearly enough room available in the
observable universe to write them down.'' That surely qualifies it
to count as galactic. 

\section{Regular expressions}
The separate chapter here on pattern matching also presents a problem
that might have seemed tame but is in fact galactic.

\section{Primality checking}
It is easy to check if a small number $N$ is prime -- just check
each potential factor up to $\sqrt N$. For larger numbers there are
fine and practially very sensible methods some of which rely on a supply
of (genuine) random numbers are deliver a result with arbitrarily small
probability of error, or that rely on currently unproven results in
number theory. But a scheme introduces by Agrawal, Kayal and Saxen\cite{AKS}
runs in time bounded by a polynominal in the number of digits it takes to
express the number and guarantees a correct response without needing to
make any qauestionalble assumptions. This is difficult in two senses!
The mathematics behind explaining and justifying it are somewhat tough,
and if it was implemented the suggestion is that its cost would would
be significantly slower than using probabilistic methods for all
numbers with no more than $10^{1000}$ digits. It is so labotious to
use that even for numbers of a few thousand bits (as used in many
cryptographic contexts) is is not really feasible at all as well as it
being hugely slower than other methods. So here we can report that this
exists, that it is a fine example of the levels of difficulty you get
when trying to get the very best solution to a problem, but that those
who want full details may first need to attend courses on number
theory!

\section{Minimum Spanning Trees}
A graph here is a collection of vertices with edges that join some of them.
A spanning sub-tree is a setset of the edges such that it is possible to
get from any vertex to any other one only traversing edges in that subgraph.
A graph may have weights (or costs) associated with each edge, and
then a minimum spanning tree is a spanning sub-tree such that the sum of the
weights on all its edges is as small as possible. Often in university courses
on datastructures and algorithms procedures by Prim and
Kruskal\cite{PrimKruskal} that find such minimum trees are presented. In
the more advanced versions of such courses improvements on the basic
versions of those using forms of priority queues known as Fibonacci
Heaps\cte{FibHeap} get discussed. If the graph hash $m$ edhes and $m$
vertices these schemes can guarantee to find a minimum spanning tree
within time proportioanl to $m + n \log n$ and that is typically good enough
for practical purposes. However a fancier method by Chazelle\cits{chazelle}
has a growth rate that scales as $m \alpha{m,n)$ where $\alpha$ is an
inverse Ackerman function and grows exceptionally slowly such that
for any conceivably feasible practical graph its value will be no
greater than 4. So until we are really at or beyond galactic scale the
costs of this method grow linearly with the number of edges regardless of
the number of vertices. As with the previous examples in this chapter this
represents a breakthrough such that for large enough graphs the method
will beat all the ones that can been developed before. As with other examples
in this chapter the details of the extreme method defy compact explanation,
but the overview is that they replace the Fibonacci heaps (which are themselved
somwhat messy!) with data structures known as ``soft heaps'' that mostly
provide priorty queue operations in a really cheap way but that
do not always yield the correct result. However they do offer bounds on
the frequency with which errors can be introduces, and the Chazelle method
as a while allows for that by making checks as it goes and where necessary
back-tracking to correct for mistakes. The presence of the inverse
Ackermnann function in the eventual cost bound gives clear warning that
this is all far from straightforward even though right here we are not
documenting just what that function is or how it arises! 
