\chapter{Practical Performance}
Galactical algorithms is all about pushing theory and assymptotic
performance estimates to the limit regardless of whether the
result would be practical. So a scheme that ran in $10^{100}N$ when
faced with a problem of size $N$ would be prefereed to one that
needed $0.001 N^{1.0001}$ because for large enough $N$ it would be the
winner. This chapter considers the other end of a spectrum and
considers cases where absolute timings measured in minutes and seconds
or space use in bytes is to be optimized, but in the pursuit of
perfection all other constraints and limitations are ignored. This
can certainly involve use of significantly messy and elaborate code
to achieve even small improvements over straightforward implementation.


\section{Abandoning portability}

% Write in machine code. Depend on word=length. Use special features
% not available on all machines (eg sse, avx etc). 
% keying data layout to cache structure and disk block sizes.

\section{Special hardware support}

% Examples are that CPU designers have put in support for some encryption
% related operations, and graphics cards are very much this. In the past
% there were different styles of comnputer for commercial and scientific
% use (and floating point hardware was not standard). In the past machines
% to support particular programming languages (eg Lisp) where built.

\section{Composite methods}

% eg a general purpose sort routinme in a library may use different methods
% for different size bodies of data, and combine multiple simple algorithms
% using parts of each at different stages of its work. 
%
% test machine characteristics and select the scheme that will work best
% here.

\section{Dynamic code generation}

% eg just in time compilation aimed at measuring what matters before
% fixing the code (eg whether branches are typically taken or not).
%
% Dynamic translation as part of emulation of one machine architecture
% by another.

