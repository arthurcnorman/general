\chapter{Intrduction}
\begin{quotation}\textit{
Computers (including phones and all manner of smart devices) have been
made so easy to use that often there is no need to think about exactly what
is going on. Even programming can be mads to seem an almost trivial
activity. Here the object is perhaps less to make computing difficult
and more to uncover some of the depth that can be found in it when one
looks just below the glossy surface.
}\end{quotation}
There are may ways in which the world tries to pretend that computing
is easy. There are schemes that teach coding to children certainly 
starting from age 6. There are self-help books with titles along the lines 
of "Teach yourself programming in <so many days>". Almost every new 
serious programming language or software package will trumpet that it
represents the next step in rendering computer use accessible to all.
Finally one of the claims for Artificial Intelligence is that it means 
that everybody can develop computer systems by merely giving an informal
explanation of what they want achieved. A rather small amount of web 
search (which is of course really easy!) will back up all the above.
But what is hidden in all that enthusiasm is that the behaviour of
computers and software and the design and construction and analysis  of 
programs has astonishing layers of difficulty just beneath the shiny
and simple-looking surface.

There are basically two reasons for investigating this difficulty. The
first can obviously arise if you are trying to build a computer-based
product or solve some particular problem and you come face to face with 
the unhappy fact that the world is messy and that naive or simplistic
techniques are not good enough. If you are an optimist this may come as a
nasty surprise! The second which is the one emphasised here is when you
understand that clever techniques, fairly intricate details and plain
weird results can be fascinating -- and that coming face to face with some 
of them will let you build experience and understanding that lets you 
achieve more in the future.

So we start off here by noticing that many computing challenges had be
presented in ways that do not need special skill or knowledge to 
appreciate. In plenty of cases there will be fairly obvious ways to start 
work towards resolving them. But then there are a dozen or more attitudes
as well as problem categories that make it possible to unpick levels of 
difficulty seriously greater than were first apparent.

In some of the examples included here even the more complicated way to
solve the problem will in fact be reasonably easy to grasp (once you have
seen why it is necessary to go to all the trouble involved). In others it 
will involve somewhat messy data-structures or mathematics -- but the 
cases we have chosen are intended to make these possible to understand and 
appreciate. Finally there are cases where there is no known solution or
(worse) where it is known that there is no perfect solution. In such cases
grasping how it is possible to demonstate that something is impossible is 
of itself a challenge worth facing up to.

So here is a sort of catalogue of ways that let you start with a simple
task and uncover the challenges concealed beneath its simplicity. For each
idea there is a reference to a section later on here that works through
an example in reasonable detail:

% The following list may need editing and it certainly needs to be
% reviewed so that the ideas presented are in the "right" order}}}

\begin{longdescription}
\item[Look beneath the abstraction to understand how something  
is implemented or how it ``really'' works]

There are an amazing number of instances of this. Any time you ask a
computer to sort some data, do a database lookep, compile a program,
create a file, fetch a web page or encrypt a message that is suposed to
be kept secret there is a great deal of technology that happily most
people can just take for granted. One can view this as rather like the
situation with almost all technology from digital watches to aeroplanes -- 
almost anybody can take advantage of them. Plenty of people will be able 
to present an overview of how and why they work. But the details end up
almost unimaginably complex. So the attitude of this point will underpin
almost all of the sections here! Here an analogy might be that leaping
on a motorbike or into a car and driving off probably seems pretty
straightforward. Understanding in full detail how the engine and
transmission works to a level where you can see how to design the
next version to further optimise power or fuel efficiency or cost of
manufacture is a very different business! For computers one can consider
details not just of the program being written, but also the compiler used
to process it, the operating system that as installed, how the instruction
set of the computer is implemented in terms of gates and other logic
circuitry down to the device physics behind the transistors inside.
Of course for {\em practical} purposes it is not generally necessary to
have specialist understanding of all the levels, but gaining it reveals
just how difficult it is to get a proper in depth understanding of
computation. A happy though here is that the very earliest computers,
while much less powerful and very much less convenient to use, were
really a lot simpler and so looking back in history makes it possible
to understand them from top to bottom much more easily. 

\item[A product vs.\ a personal project]
This is perhaps best explained by an analogy with feature films.
One could dream that anybody with a modern phone could capture video and
a few friends could take parts in film. The status of the cameras on current
phones which are inexpensive (at least in comparison with professional gear!)
and can deliver remarkably high quality is analagous to the way in
which these days computers are cheap, powerful and can so an amazing range of
things following just a few clicks on an app or a web-resource.
By choosing the plot you would not need any elaborate scenery or props,
and you can avoid the need for potentially dangerous stunts.
Some films pretend to be constructed by pasting together ``found footage''
extracted from the phones of characters (who will normally have vanished
or died) further boosting the illusion that almost no serious equipment
is needed. To get a clearer view of reality look at the list of credits
at the end of a film and see the size of team really involved in even a
seemingly simple one. Check the budget involved and let that give another
indication of the complexity of the full process -- even before the issues
of marketing and distribution are taken into account. So the reality is that
in general the finished product almost conceals the fact that it relies on the
work of hundreds anmd sometimes thousands of people: scouting for locations,
make up artists, lighting technicians, foley artists who add sound effects
and many many more down to the transport catering teams who support the
people more on the front line. In general while being a member of an audience
you are almost unaware of all of this -- save that if it was missing that
would be very noticable. This situation can seen as an ``iceberg syndrome''
where something that looks simple and where indeed an individual amateur
could start to develop something in fact relies on such a wide range
of techniques and skills that for any one person to master them all
would represent a huge challenge. The message here is that making a
a home-movie is amazingly easy, but the simpliciy there conceals the fact
there is a great deal of difficulty in creatiing a block-buster. The
same sort of things applies as between a bit of software created
just for your own private use and a full-blown product.


\item[Consideration of edge and worst cases]

There is a scheme called ``Newton's Method'' or ``The Newton-Raphson 
Iteration'' that provides a simple to implement way of obtaining numeric
solutions to equations. As a concrete and rather easy example it can be 
used on the equation $x^3-a=0$ for some known value of $a$ to find the
cube root of $a$. But even with a case as easy as this there are challenges.
How fast will it get an accurate result? It needs an initial guess for its
answer to start from -- how much does that value matter. An especially
jolly issue for this case is that if one accepts that in many areas of 
mathemetics and physics one is working with complex rather than real 
numbers it is necessary to accept that $a$ will have three cube roots -- and
the issue of which one Newton's method will deliver for you turns out to 
be a messier issue than you might have expected.

There are plenty of non-numeric instances of difficulty raised by asking
about best and worst (and indeed average) cases in problems. For instance
solving a Sudoku puzzle or planning how to return Rubic's cube to a tidy
state may not be totally trivial, but trying to identify the hardest 
possible Sudoku board or the most awkward starting point for a Rubic's 
cube escalates the challenge sharply!
Yet another example here come from ``turtle graphics'' -- a computational 
model that has often been used in introductions of computers to the very 
young. In that world it can be quite easy to ask such questions as ``If you 
continue with this pattern of movement will you ever find yourself exactly 
back where you began?'' or ``Might your turtle eventually fall off the end 
of the paper or indeed the world?'' that make life tougher for yourself.


\item[Insist on total correctness in every case.]

Pocket calculators and computers provide facilities to calculate 
trigonometric functions, logarithms and square roots of numbers. In normal
circumstances one just truists the computer. But there are two ways to
make your own life harder. One is to express a fear that the computer will
occasionally get bad results. Well in 1997 there were significant sales of 
computers that did not even always get division correct! How do you test
things like this?

To go further note that with a computer the answers will always have been
clipped to some limited precision. On a pocket calculator this may for
instance be 8 decimal digits. The full perfact answer to a calculation has
digits beyond that - so for instance if $\pi$ is returned as $3.1415926$ on
a calculator (which can feel reasonable) there is an issue in that the
true value is $3.1415626_5359\ldots$ and the value quoted should have been 
rounded up because if the $5359\ldots$ tail beyond where it ends. So the
challenge is to evaluate all the elementary functions in such a way that
for all possible inputs the result that is returned is correctly rounded 
-- ie as accurate as is at all possible. And for this to be done first 
without intolerable extra cost and secondly with some proper scheme that 
can certify that the goal of perfection has been achieved. Amazingly there 
are people who have been arranging that! A few years ago one could almost
always just assume that the computer's results would be more than precise
enough for all reasonable purposes, but now people are using rather
low precision floating point arithmetic for big parallel comutations in
either graphics or artifical intelligence areas, and so these rather pedantic
issues of precision come much closer to pracical reality.

\item[Portability]

For small programs that are only intended for use over a fairly short
perious of time and only by one person on their own computer it is
not necessary to worry about portability. However larger projects
raise more and more serious challenges. Windows, Macintosh, Android and
Linux are amazingly not 100\% compatible with each other, and it is often
necessary to use techniques that apply to just one particular system.
Intel and AMD processors, ARM and Risc-V are all different and are either
at present in widespread use or may become so. There are areas where each
of those needs custom treatment to achieve goals even as simple sounding as
measuring time with the highest feasible precision. Use of a sufficiently
high level language can conceal these issues by arranging that all the mess
is embedded within the language and its associated library - but for our
purposes that still leave curious people with a need to know exactly what
is going on. And very frequently common solutions that paper over differences
carry costs that oen might like to avoid.

\item[Demand the fastest (or most compact) solution that could  
ever exist.]

The world of computer gaming is one where delivering the fastest 
frame-rate for the highest resoluation version of the action ia of serious 
commercial importance. And that is a matter of real directly measurable 
performance where there are basicelly few limits to what can be deployed 
to deliver it. That can lead to power-hungry and expensive video cards 
with amazingly elaborate driver software. Because much of that world is 
proprietary it is perhaps hard to get into, but it does illustrate that 
despite the fact that computers have become quite fast there are still 
areas where squeezing the best from them matters. This can involve both 
algorithm selection and coding style. For some application areas this aim
to excel has the same sort of issues that mean that it will be different
sets of athletes who dominate over 100 metres and over the 42195 metres of 
a marathon. In computing sometimes techniques that will be great for large 
problem instances will not show up well on smaller inputs. There are two 
classical illustrations of this to be found in books on computer 
algorithms. One related to finding all then shortest routes from a given 
starting point to other locations in a maze (which is generally referred 
to as a graph). The other is simply multiplying numbers together. We will
summarise and discuss each of these and various other cases where 
optimising for speed or size turns a reasonable problem into a tough one.

One nice term sometimes used is ``galactic algorithms''. This is used
to refer to ways of solving problems that in the long term would be
better or faster than the simpler methods more commonly used, but achieve
their closer approach to optimality at the cost of impressive overheads.
Sometimes by way of a constant factor slowdown that would not by overcome
by the fact that their cost grows a little more slowly than the standard
schemes, and sometimes by such a level of implmentation complication and
mess that it will basically never make sense for some rather minor
improvement. A few examples of this style will be covered here. The
term ``galactic'' is to suggest something of the magnitude of problem
instances where some of these would necome competitive. 

\item[Challenging underpinning theory behind a simple-looking problem]
I do not have nice text here, just a list of a few 
\begin{enumerate}
\item Simplify algebraic formulae.
\item Telling if it is possible to find values for all the variables to
make a boolean formula evaluate to TRUE.
\item the 3n+1 challenge (ie Collatz).
\item Generate an unpredictable (ie random) sequence.
\end{enumerate}

\item[Constraints that rule out obvious approaches]

In some real sense all computer projects fall into this category because 
if you had a programming language or software package that aligned well 
enough with your task then everything would become easy. So to illustrate 
the point we will cover cases of extremely weak programming environments 
where it might not be obvious to start with that it will be possible to do
anything much of interest, Each of the ones listed here and delved into 
in a bit more depth later can give insight into some particular aspect of 
computation:
\begin{enumerate}
   \item Turing Machines and variants
   \item Counter machines
   \item Lambda-calculus and combinators
   \item Cellular automata
   \item Primitive recursive functions
\end{enumerate}
Several of these start off seeming to be rather abstract and certainly not
obviously practical, but for instance a highlight in one case is a report 
of how somebody has built a computer out of lego based on one of them such 
that in principle it is fully general purpose!

\item[Seek good partial solutions to intractable problems]

Some -- indeed many -- of the challenges that arise in the real world turn
out to be such that nobody knows how to solve them in general and there is
serious reason to believe that that will always be the case. In a number 
of these it is even possible to prove that no general solution can be 
found. That opens the door to a lot of fun seeking either computer schemes
that obtain approximate solutions or ones that sometimes or perhaps often 
get to the best answer, but are not guaranteed to complete the task. WQe 
provide several examples!

\item[Use wilfully perverse techniques that achieve your goal.]

There can be great joy in exploring stupid ways of solving problems -- and 
sometimes these emerge when a naive programmer submits their best efforts 
and you recognize that what they have achieved is a program that works but 
is magnificantly slower than one might have hoped. Even experienced
software engineers can end up delivering solutions that in retrospect can 
be seen to be pretty well absurd. We can illustrate this with cases from
simple tasks the like of which are set as exercises in an elementary 
programming class: reversing a list, sorting some data and the like.

\item[Challenges that make good puzzles]

Would any reasonable person want to invent unrealistic or pointless tasks? 
Why yes - those involved in recruitment of any sort might want "puzzle 
tasks" to set to candidates and while some questions they pose might 
merely test depth of knowledge, others want to look for inventiveness and 
the ability to think on the feet. Perhaps giving a preview of such
cases undermines the joy in them, and maybe it will even be hard to tell
which of the sections here have a component of this philosophy!

\item[Future-proofing]

The landscape of computers has changed dramatically, and today there are 
still big new developments in prospect. Perhaps the two most visible are
artificial intelligence and quantum computation, but one should really 
also note that the exploitation of the various forms of massive 
parallelism that is now available respresents a frontier. While some of 
the underpinning theoretical study of computation has remained valid for a 
long while, much practical software has a lifespan of a rather few 
decades. And segments of the theory that were central to all courses on 
computer science a generation back have much less clear-cut relevance than 
before. So we provide a few case studies that note both exciting ideas 
whose time seems to have passes and projects that against the odds have 
been kept alive.

\item[Emphasize what must not happen as much as what will]

When specifying what a computer system will be for it is normal to 
describe what it will achieve. However the reality of things is that
computer programs of any non-trivial size are liable to have flaws and 
will not always behave as intended. So there are cases where it becomes
important to specify what must not happen as well as what should. And 
indeed in somc cases the negatives are actually the important constraints.
We will cover two areas that highlight this: security and safety-critical 
systems. The first can usefully be partitioned into the consideratiuon of 
first encryption primitves (ie codes and the like) and then into protocols 
(how information is exchanged reliably between several parties). The 
counterpart to designing and building secure system is defeating same - 
and there is an amazing history of purportedly safe schemes being cracked 
open!

\item[Cope with the inevitability of human error]
We all know that ``To err is human'', and we are all used to observing that
computer systems have flaws. If you were concerned with safety critical
applications (think of control of anti-locking brakes on a car, of
the guidance system for a missile or for a device to be implanted in
somebody's body) you may feel the need to reduce the chances of error
as far as you possibly can. You may also wish to be resilient against
all possibe forms of hardware malfunction. There is no simple silver
bullet.

Both the hardware of computers and the software they run will be
constructed by humans -- or these days perhaps by an artificially
intelligent agent. Anybody who claims that what they deliver has been
built by some spcial and really reliable tool shoudl be asked about
who created that tool and what basis there is for thinking its is perfact.

The inevitable result is that at least the initial version of any software
must be presumed to be flawed. Testing -- even very extensive testing --
tends not to uncover all the defects. There are two approaches that aim
toward perfection: (a) look for software building techniques that minimise
(note  "minimise" rather than "eliminate") errors and (b) investigate ways
to create formal proofs that the software ends up correct. We will have 
examples of or introductions to each of these.


\item[Make sense of abstract graphical presentations]
Various schemes generate amazingly complicated images from rather 
simple recipes. Many people have come across the Mandelbrot Set which is 
one such instance, but we will point at a number more and consider how 
much understanding can be gleaned from reviewing the pictures.

\end{longdescription}

Obviously these schemes for making life harder overlap in places, but it
is also the case that several can all apply at the same time. The result
is that challenges that started off seeming easy can end up causing quite
severe headaches.

One might reasonably ask whether real people apply any of these
principles in their lives. The next chapter sketches a particularly
extreme case, but in reality a great deal of work starts with tasks
that turn out to have depths that were not at all obvious at the start.

So it is very much the case that even though computing can be seen as
really simple and we can keep telling ourselves ``The computer only
does what you tell it to'', just beneath that attractive simple veneer
there are a great many wonderful, messy, complicated and difficult details.

% End of intro.tex



